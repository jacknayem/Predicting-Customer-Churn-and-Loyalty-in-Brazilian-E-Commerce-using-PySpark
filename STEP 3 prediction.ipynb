{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8c121058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import count, col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType, LongType, FloatType, BooleanType, NumericType\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.classification import LinearSVC, DecisionTreeClassifier, NaiveBayes\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Olist Customer Churn\") \\\n",
    ".config(\"spark.driver.memory\", \"4g\") \\\n",
    ".config(\"spark.executor.memory\", \"4g\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "91e347c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+---------+--------+--------+--------------------+-----+------------------+------------------+------------+-----------+--------------------+--------------------+----------------+----------------+----------------+-------------------+-----+\n",
      "|  customer_unique_id|recency|frequency|monetary|zip_code|                city|state|total_items_volume|max_delivery_delay|is_delivered|is_approved|payment_method_count|primary_payment_type|max_installments|avg_satisfaction|min_review_score|total_reviews_given|label|\n",
      "+--------------------+-------+---------+--------+--------+--------------------+-----+------------------+------------------+------------+-----------+--------------------+--------------------+----------------+----------------+----------------+-------------------+-----+\n",
      "|0006fdc98a402fceb...|    232|        1|    29.0|   29400|       mimoso do sul|   ES|                 1|               -12|           1|          1|                   1|         credit_card|               2|             3.0|               3|                  1|    1|\n",
      "|000c8bdb58a29e711...|     85|        1|    29.0|   31555|      belo horizonte|   MG|                 1|                -9|           1|          1|                   1|         credit_card|               2|             5.0|               5|                  1|    1|\n",
      "|001f3c4211216384d...|    353|        1|   35.84|    8240|           sao paulo|   SP|                 1|               -11|           1|          1|                   1|         credit_card|               3|             3.0|               3|                  1|    1|\n",
      "|002cdf87d4c03f08f...|    127|        1|  228.67|   13183|         hortolandia|   SP|                 1|                14|           1|          1|                   1|         credit_card|              10|             1.0|               1|                  1|    1|\n",
      "|002d71b244beb91ca...|    289|        1|  130.56|   18900|santa cruz do rio...|   SP|                 1|               -12|           1|          1|                   1|              boleto|               1|             5.0|               5|                  1|    1|\n",
      "+--------------------+-------+---------+--------+--------+--------------------+-----+------------------+------------------+------------+-----------+--------------------+--------------------+----------------+----------------+----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================\n",
    "# Load Dataset\n",
    "# ===================\n",
    "df_path = r\"data\\finalData\\customer_training.csv\"\n",
    "\n",
    "df = spark.read.csv(\n",
    "    df_path,\n",
    "    header= True,\n",
    "    inferSchema= True,\n",
    "    multiLine=True,\n",
    "    escape='\"',\n",
    "    quote='\"'\n",
    ")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc83d24",
   "metadata": {},
   "source": [
    "## Handle Irrelevant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a842db98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ID-like columns to drop:\n",
      "['customer_unique_id']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Check ID-like columns (all values unique)\n",
    "# ============================================\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "row_count = df.count()\n",
    "\n",
    "id_like_cols = [\n",
    "    c for c in df.columns\n",
    "    if df.select(F.countDistinct(c)).collect()[0][0] == row_count\n",
    "]\n",
    "\n",
    "print(\"\\nID-like columns to drop:\")\n",
    "print(id_like_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "83b79510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Constant columns to drop:\n",
      "['is_approved']\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Check constant columns\n",
    "# ===================================\n",
    "\n",
    "distinct_counts = df.agg(*[\n",
    "    F.countDistinct(c).alias(c) for c in df.columns\n",
    "]).collect()[0].asDict()\n",
    "\n",
    "constant_cols = [c for c, v in distinct_counts.items() if v == 1]\n",
    "\n",
    "print(\"\\nConstant columns to drop:\")\n",
    "print(constant_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5f94c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name          | Unique Count | Ratio %   \n",
      "--------------------------------------------------\n",
      "customer_unique_id   | 55245        | 100.00    % (DROPPED)\n",
      "recency              | 366          | 0.66      % (KEEP)\n",
      "frequency            | 8            | 0.01      % (KEEP)\n",
      "monetary             | 18539        | 33.56     % (DROPPED)\n",
      "zip_code             | 13131        | 23.77     % (DROPPED)\n",
      "city                 | 3542         | 6.41      % (DROPPED)\n",
      "state                | 27           | 0.05      % (KEEP)\n",
      "total_items_volume   | 17           | 0.03      % (KEEP)\n",
      "max_delivery_delay   | 62           | 0.11      % (KEEP)\n",
      "is_delivered         | 2            | 0.00      % (KEEP)\n",
      "is_approved          | 1            | 0.00      % (KEEP)\n",
      "payment_method_count | 19           | 0.03      % (KEEP)\n",
      "primary_payment_type | 4            | 0.01      % (KEEP)\n",
      "max_installments     | 22           | 0.04      % (KEEP)\n",
      "avg_satisfaction     | 32           | 0.06      % (KEEP)\n",
      "min_review_score     | 5            | 0.01      % (KEEP)\n",
      "total_reviews_given  | 9            | 0.02      % (KEEP)\n",
      "\n",
      "Final list of columns to drop: ['customer_unique_id', 'monetary', 'zip_code', 'city']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Check High Cardinality\n",
    "# ==========================================\n",
    "total_rows = df.count()\n",
    "high_card_cols = []\n",
    "\n",
    "print(f\"{'Column Name':<20} | {'Unique Count':<12} | {'Ratio %':<10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col_name in df.columns:\n",
    "    # Skip the target label\n",
    "    if col_name == 'label': continue\n",
    "    \n",
    "    unique_count = df.select(col_name).distinct().count()\n",
    "    ratio = (unique_count / total_rows) * 100\n",
    "    \n",
    "    if unique_count > 50 and ratio > 1.0:\n",
    "        high_card_cols.append(col_name)\n",
    "        print(f\"{col_name:<20} | {unique_count:<12} | {ratio:<10.2f}% (DROPPED)\")\n",
    "    else:\n",
    "        print(f\"{col_name:<20} | {unique_count:<12} | {ratio:<10.2f}% (KEEP)\")\n",
    "\n",
    "print(\"\\nFinal list of columns to drop:\", high_card_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3cb9b1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature A            | Feature B            | Correlation\n",
      "-------------------------------------------------------\n",
      "frequency            | total_reviews_given  | 0.8642\n",
      "avg_satisfaction     | min_review_score     | 0.9957\n",
      "\n",
      "Final suggested columns to drop (Redundant): ['min_review_score', 'total_reviews_given']\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Check  Multicollinearity\n",
    "# =========================================\n",
    "\n",
    "# STEP 1: Identify Numeric Columns - EXCLUDING the label\n",
    "numeric_types = (IntegerType, DoubleType, LongType, FloatType)\n",
    "feature_cols = [f.name for f in df.schema.fields \n",
    "                if isinstance(f.dataType, numeric_types) and f.name != 'label']\n",
    "\n",
    "# STEP 2: Compute Correlation Matrix\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"corr_features\", handleInvalid=\"skip\")\n",
    "df_vector = assembler.transform(df).select(\"corr_features\")\n",
    "\n",
    "matrix = Correlation.corr(df_vector, \"corr_features\").head()[0]\n",
    "corr_matrix = matrix.toArray()\n",
    "\n",
    "# STEP 3: Identify Highly Correlated Pairs\n",
    "high_corr_cols = set()\n",
    "n = len(feature_cols)\n",
    "\n",
    "print(f\"{'Feature A':<20} | {'Feature B':<20} | {'Correlation'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(i + 1, n):\n",
    "        corr_val = abs(float(corr_matrix[i, j]))\n",
    "        if corr_val > 0.80:\n",
    "            print(f\"{feature_cols[i]:<20} | {feature_cols[j]:<20} | {corr_val:.4f}\")\n",
    "            high_corr_cols.add(feature_cols[j])\n",
    "\n",
    "print(\"\\nFinal suggested columns to drop (Redundant):\", list(high_corr_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d892f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Name               | Variance     | Status\n",
      "--------------------------------------------------\n",
      "recency                   | 10214.420772 | OK\n",
      "frequency                 | 0.040364     | OK\n",
      "monetary                  | 48468.659007 | OK\n",
      "zip_code                  | 892344667.024462 | OK\n",
      "total_items_volume        | 0.368729     | OK\n",
      "max_delivery_delay        | 115.807551   | OK\n",
      "is_delivered              | 0.022443     | OK\n",
      "is_approved               | 0.000000     | DROP\n",
      "payment_method_count      | 0.175390     | OK\n",
      "max_installments          | 7.714447     | OK\n",
      "avg_satisfaction          | 1.818769     | OK\n",
      "min_review_score          | 1.851334     | OK\n",
      "total_reviews_given       | 0.075351     | OK\n",
      "\n",
      "Final list of redundant low-variance columns: ['is_approved']\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Check Low-Variance\n",
    "# ==========================================\n",
    "\n",
    "# STEP 1: Identify Numeric Columns - EXCLUDING the label\n",
    "numeric_types = (IntegerType, DoubleType, LongType, FloatType)\n",
    "# Safety: Don't check the label for variance thresholding\n",
    "cols_to_test = [f.name for f in df.schema.fields \n",
    "                if isinstance(f.dataType, numeric_types) and f.name != 'label']\n",
    "\n",
    "# STEP 2: Calculate Variance\n",
    "variance_df = df.select([F.variance(c).alias(c) for c in cols_to_test])\n",
    "variance_dict = variance_df.first().asDict()\n",
    "\n",
    "# STEP 3: Identify Low-Variance Columns (threshold = 0.01)\n",
    "threshold = 0.01\n",
    "low_variance_cols = []\n",
    "\n",
    "print(f\"{'Column Name':<25} | {'Variance':<12} | {'Status'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for col, var in variance_dict.items():\n",
    "    if var is not None and var <= threshold:\n",
    "        low_variance_cols.append(col)\n",
    "        print(f\"{col:<25} | {var:<12.6f} | DROP\")\n",
    "    else:\n",
    "        print(f\"{col:<25} | {var:<12.6f} | OK\")\n",
    "\n",
    "print(f\"\\nFinal list of redundant low-variance columns: {low_variance_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2e4bb915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns to drop: 6\n",
      "Columns to be removed: ['customer_unique_id', 'zip_code', 'city', 'is_approved', 'min_review_score', 'total_reviews_given']\n",
      "+-------+---------+--------+-----+------------------+------------------+------------+--------------------+--------------------+----------------+----------------+-----+\n",
      "|recency|frequency|monetary|state|total_items_volume|max_delivery_delay|is_delivered|payment_method_count|primary_payment_type|max_installments|avg_satisfaction|label|\n",
      "+-------+---------+--------+-----+------------------+------------------+------------+--------------------+--------------------+----------------+----------------+-----+\n",
      "|    232|        1|    29.0|   ES|                 1|               -12|           1|                   1|         credit_card|               2|             3.0|    1|\n",
      "|     85|        1|    29.0|   MG|                 1|                -9|           1|                   1|         credit_card|               2|             5.0|    1|\n",
      "|    353|        1|   35.84|   SP|                 1|               -11|           1|                   1|         credit_card|               3|             3.0|    1|\n",
      "|    127|        1|  228.67|   SP|                 1|                14|           1|                   1|         credit_card|              10|             1.0|    1|\n",
      "|    289|        1|  130.56|   SP|                 1|               -12|           1|                   1|              boleto|               1|             5.0|    1|\n",
      "+-------+---------+--------+-----+------------------+------------------+------------+--------------------+--------------------+----------------+----------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cols_to_drop_set = set(id_like_cols) | set(high_card_cols) | set(high_corr_cols) | set(low_variance_cols)\n",
    "cols_to_drop_set.discard('monetary')\n",
    "\n",
    "cols_to_drop = list(cols_to_drop_set)\n",
    "\n",
    "print(f\"Total columns to drop: {len(cols_to_drop)}\")\n",
    "print(f\"Columns to be removed: {cols_to_drop}\")\n",
    "\n",
    "df_final = df.drop(*cols_to_drop)\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f02415bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: integer (nullable = true)\n",
      " |-- monetary: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- total_items_volume: integer (nullable = true)\n",
      " |-- max_delivery_delay: integer (nullable = true)\n",
      " |-- is_delivered: integer (nullable = true)\n",
      " |-- payment_method_count: integer (nullable = true)\n",
      " |-- primary_payment_type: string (nullable = true)\n",
      " |-- max_installments: integer (nullable = true)\n",
      " |-- avg_satisfaction: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "40b56af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Strategy Results ---\n",
      " Normalize (Standard/Robust/MinMax): ['recency', 'monetary', 'max_delivery_delay']\n",
      " Label Encode (StringIndexer): []\n",
      " One-Hot Encode (StringIndexer + OneHotEncoder): ['state', 'primary_payment_type']\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Analyse Columns to Normalize and Encode\n",
    "# =========================================\n",
    "\n",
    "def analyze_encoding(df_final, target=None):\n",
    "    normalize_cols = []\n",
    "    label_encode_cols = []\n",
    "    one_hot_encode_cols = []\n",
    "    \n",
    "    \n",
    "    # 2. Define numeric type prefixes for PySpark\n",
    "    numeric_prefixes = ('int', 'double', 'float', 'long', 'decimal', 'short')\n",
    "    \n",
    "    # 3. Iterate through columns\n",
    "    for col_name, dtype in df_final.dtypes:\n",
    "        if col_name == target:\n",
    "            continue  # Skip target column\n",
    "            \n",
    "        # Get unique count (using distinct().count())\n",
    "        unique_vals = df_final.select(col_name).distinct().count()\n",
    "            \n",
    "        if dtype.startswith(numeric_prefixes):\n",
    "            # Numeric columns\n",
    "            if unique_vals > 50:\n",
    "                normalize_cols.append(col_name)\n",
    "        else:\n",
    "            # Categorical columns\n",
    "            if unique_vals == 2:\n",
    "                label_encode_cols.append(col_name)\n",
    "            elif unique_vals <= 30:\n",
    "                one_hot_encode_cols.append(col_name)\n",
    "            else:\n",
    "                print(f\" Warning: High-cardinality categorical column: {col_name} ({unique_vals} unique values)\")\n",
    "                \n",
    "    return normalize_cols, label_encode_cols, one_hot_encode_cols\n",
    "\n",
    "target_col = 'label'\n",
    "norm_cols, label_cols, onehot_cols = analyze_encoding(df_final, target=target_col)\n",
    "\n",
    "print(\"\\n--- Strategy Results ---\")\n",
    "print(\" Normalize (Standard/Robust/MinMax):\", norm_cols)\n",
    "print(\" Label Encode (StringIndexer):\", label_cols)\n",
    "print(\" One-Hot Encode (StringIndexer + OneHotEncoder):\", onehot_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4d4f4013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Optimized Scaler Analysis ---\n",
      "Avg Skewness: 4.25\n",
      "Avg Outlier Ratio: 0.0521\n",
      "Final Decision: MINMAX\n"
     ]
    }
   ],
   "source": [
    "def suggest_scaler_optimized(df, numeric_cols):\n",
    "    total_count = df.count()\n",
    "    if total_count == 0: return \"standard\"\n",
    "\n",
    "    # STEP 1: CALCULATE ALL SKEWNESS IN ONE PASS\n",
    "    skew_expressions = [F.skewness(c).alias(c) for c in numeric_cols]\n",
    "    skews = df.select(skew_expressions).first().asDict()\n",
    "    \n",
    "    # STEP 2: CALCULATE ALL QUANTILES IN ONE PASS\n",
    "    all_quantiles = df.approxQuantile(numeric_cols, [0.25, 0.5, 0.75], 0.01)\n",
    "    \n",
    "    quantile_map = {col: all_quantiles[i] for i, col in enumerate(numeric_cols)}\n",
    "\n",
    "    # STEP 3: CALCULATE ALL OUTLIER COUNTS IN ONE PASS\n",
    "    outlier_expressions = []\n",
    "    for col in numeric_cols:\n",
    "        q1, median, q3 = quantile_map[col]\n",
    "        iqr = q3 - q1\n",
    "        if iqr > 0:\n",
    "            # Standard Tukey's Fences: x < Q1 - 1.5*IQR or x > Q3 + 1.5*IQR\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            outlier_expressions.append(\n",
    "                F.sum(F.when((F.col(col) < lower_bound) | (F.col(col) > upper_bound), 1).otherwise(0)).alias(col)\n",
    "            )\n",
    "        else:\n",
    "            outlier_expressions.append(F.lit(0).alias(col))\n",
    "\n",
    "    outlier_counts = df.select(outlier_expressions).first().asDict()\n",
    "\n",
    "    # STEP 4: DECISION LOGIC\n",
    "    avg_skew = np.mean([abs(v) for v in skews.values() if v is not None])\n",
    "    avg_outlier_ratio = np.mean([v / total_count for v in outlier_counts.values()])\n",
    "\n",
    "    print(f\"--- Optimized Scaler Analysis ---\")\n",
    "    print(f\"Avg Skewness: {avg_skew:.2f}\")\n",
    "    print(f\"Avg Outlier Ratio: {avg_outlier_ratio:.4f}\")\n",
    "\n",
    "    if avg_outlier_ratio > 0.15:\n",
    "        return \"robust\"\n",
    "    elif avg_skew > 1.0:\n",
    "        return \"minmax\"\n",
    "    else:\n",
    "        return \"standard\"\n",
    "\n",
    "# --- Execution ---\n",
    "scaler_name = suggest_scaler_optimized(df_final, norm_cols)\n",
    "print(f\"Final Decision: {scaler_name.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1703c1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ohter Features: ['frequency', 'total_items_volume', 'is_delivered', 'payment_method_count', 'max_installments', 'avg_satisfaction']\n"
     ]
    }
   ],
   "source": [
    "label_col = 'label'\n",
    "\n",
    "numeric_cols = [\n",
    "    f.name for f in df_final.schema.fields\n",
    "    if isinstance(f.dataType, NumericType)\n",
    "]\n",
    "\n",
    "other_numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in norm_cols and col != label_col\n",
    "]\n",
    "print(f\"Ohter Features: {other_numeric_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "89ddd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Split data into train and test\n",
    "# ==============================\n",
    "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b62fc9",
   "metadata": {},
   "source": [
    "### Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac2e6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================\n",
    "# One-Hot Encode\n",
    "# ==========================================\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col, outputCol=f\"{col}_index\", handleInvalid=\"keep\")\n",
    "    for col in onehot_cols\n",
    "]\n",
    "\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col}_index\", outputCol=f\"{col}_ohe\")\n",
    "    for col in onehot_cols\n",
    "]\n",
    "\n",
    "#===========================================\n",
    "# Normilization\n",
    "# ==========================================\n",
    "\n",
    "norm_assembler = VectorAssembler(\n",
    "    inputCols=norm_cols,\n",
    "    outputCol=\"norm_vector\"\n",
    ")\n",
    "\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol=\"norm_vector\",\n",
    "    outputCol=\"norm_features\"\n",
    ")\n",
    "\n",
    "#===========================================\n",
    "# Assemble ALL features into final vector\n",
    "# ==========================================\n",
    "final_assembler = VectorAssembler(\n",
    "    inputCols=\n",
    "        ['norm_features'] +\n",
    "        other_numeric_cols +\n",
    "        [f\"{col}_ohe\" for col in onehot_cols],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# --------- Execution -------------\n",
    "pipeline = Pipeline(stages=[\n",
    "    # Categorical\n",
    "    *indexers,\n",
    "    *encoders,\n",
    "\n",
    "    # Normalize selected numeric features\n",
    "    norm_assembler,\n",
    "    scaler,\n",
    "\n",
    "    # Assemble EVERYTHING\n",
    "    final_assembler\n",
    "])\n",
    "\n",
    "pipeline_model = pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "91a7fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transformed = pipeline_model.transform(train_df)\n",
    "test_transformed  = pipeline_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de3f859",
   "metadata": {},
   "source": [
    "**Check Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7ede7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: integer (nullable = true)\n",
      " |-- monetary: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- total_items_volume: integer (nullable = true)\n",
      " |-- max_delivery_delay: integer (nullable = true)\n",
      " |-- is_delivered: integer (nullable = true)\n",
      " |-- payment_method_count: integer (nullable = true)\n",
      " |-- primary_payment_type: string (nullable = true)\n",
      " |-- max_installments: integer (nullable = true)\n",
      " |-- avg_satisfaction: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- state_index: double (nullable = false)\n",
      " |-- primary_payment_type_index: double (nullable = false)\n",
      " |-- state_ohe: vector (nullable = true)\n",
      " |-- primary_payment_type_ohe: vector (nullable = true)\n",
      " |-- norm_vector: vector (nullable = true)\n",
      " |-- norm_features: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "37c51891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+------------------------+\n",
      "|primary_payment_type|primary_payment_type_index|primary_payment_type_ohe|\n",
      "+--------------------+--------------------------+------------------------+\n",
      "|boleto              |1.0                       |(4,[1],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|boleto              |1.0                       |(4,[1],[1.0])           |\n",
      "|credit_card         |0.0                       |(4,[0],[1.0])           |\n",
      "|boleto              |1.0                       |(4,[1],[1.0])           |\n",
      "+--------------------+--------------------------+------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "train_transformed.select(\"primary_payment_type\", \"primary_payment_type_index\", \"primary_payment_type_ohe\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b972886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------------+\n",
      "|state|state_index|state_ohe     |\n",
      "+-----+-----------+--------------+\n",
      "|MG   |2.0        |(27,[2],[1.0])|\n",
      "|SP   |0.0        |(27,[0],[1.0])|\n",
      "|SP   |0.0        |(27,[0],[1.0])|\n",
      "|SP   |0.0        |(27,[0],[1.0])|\n",
      "|SP   |0.0        |(27,[0],[1.0])|\n",
      "|MG   |2.0        |(27,[2],[1.0])|\n",
      "|RJ   |1.0        |(27,[1],[1.0])|\n",
      "|MG   |2.0        |(27,[2],[1.0])|\n",
      "|PR   |4.0        |(27,[4],[1.0])|\n",
      "|DF   |8.0        |(27,[8],[1.0])|\n",
      "+-----+-----------+--------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "train_transformed.select(\"state\", \"state_index\", \"state_ohe\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cbf52c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------------------------------------+\n",
      "|norm_vector     |norm_features                                  |\n",
      "+----------------+-----------------------------------------------+\n",
      "|[0.0,30.88,-4.0]|[0.0,0.00141000333273515,0.42622950819672134]  |\n",
      "|[0.0,33.39,-5.0]|[0.0,0.0015938531179385382,0.4098360655737705] |\n",
      "|[0.0,40.23,-2.0]|[0.0,0.0020948620943493654,0.45901639344262296]|\n",
      "|[0.0,45.29,-7.0]|[0.0,0.0024654915418111764,0.3770491803278689] |\n",
      "|[0.0,47.51,-6.0]|[0.0,0.0026280997183655676,0.39344262295081966]|\n",
      "+----------------+-----------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "train_transformed.select(\"norm_vector\", \"norm_features\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "02f96e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                               |label|\n",
      "+-------------------------------------------------------------------------------------------------------+-----+\n",
      "|(40,[1,2,3,4,5,6,7,8,11,37],[0.00141000333273515,0.42622950819672134,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0]) |1    |\n",
      "|(40,[1,2,3,4,5,6,7,8,9,36],[0.0015938531179385382,0.4098360655737705,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0]) |1    |\n",
      "|(40,[1,2,3,4,5,6,7,8,9,36],[0.0020948620943493654,0.45901639344262296,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0])|1    |\n",
      "|(40,[1,2,3,4,5,6,7,8,9,36],[0.0024654915418111764,0.3770491803278689,1.0,1.0,1.0,1.0,1.0,5.0,1.0,1.0]) |1    |\n",
      "|(40,[1,2,3,4,5,6,7,8,9,36],[0.0026280997183655676,0.39344262295081966,1.0,1.0,1.0,1.0,4.0,3.0,1.0,1.0])|1    |\n",
      "+-------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "train_transformed.select(\"features\", \"label\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a14387f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector size: 40\n",
      "Non-zero indices: [ 1  2  3  4  5  6  7  8 11 37]\n",
      "Non-zero values: [1.41000333e-03 4.26229508e-01 1.00000000e+00 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00 1.00000000e+00 5.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Pick the first row\n",
    "row = train_transformed.select(\"features\").first()[0]\n",
    "\n",
    "print(\"Vector size:\", row.size)\n",
    "print(\"Non-zero indices:\", row.indices)\n",
    "print(\"Non-zero values:\", row.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298b077",
   "metadata": {},
   "source": [
    "**Why I choose \"Class Weights\" instaed of \"Downsampling3 or \"Oversampling\"**\n",
    "\n",
    "Instead of deleting data (Downsampling) or making up fake data (Oversampling), the most professional way to handle this in PySpark is to use **Class Weights**. This allows me to **keep all ~44,000 rows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f1994520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-----+------------------+------------------+------------+--------------------+--------------------+----------------+----------------+-----+-----------+--------------------------+--------------+------------------------+----------------+--------------------+--------------------+-----------------+\n",
      "|recency|frequency|monetary|state|total_items_volume|max_delivery_delay|is_delivered|payment_method_count|primary_payment_type|max_installments|avg_satisfaction|label|state_index|primary_payment_type_index|     state_ohe|primary_payment_type_ohe|     norm_vector|       norm_features|            features|           weight|\n",
      "+-------+---------+--------+-----+------------------+------------------+------------+--------------------+--------------------+----------------+----------------+-----+-----------+--------------------------+--------------+------------------------+----------------+--------------------+--------------------+-----------------+\n",
      "|      0|        1|   30.88|   MG|                 1|                -4|           1|                   1|              boleto|               1|             5.0|    1|        2.0|                       1.0|(27,[2],[1.0])|           (4,[1],[1.0])|[0.0,30.88,-4.0]|[0.0,0.0014100033...|(40,[1,2,3,4,5,6,...|0.505940503222551|\n",
      "|      0|        1|   33.39|   SP|                 1|                -5|           1|                   1|         credit_card|               1|             5.0|    1|        0.0|                       0.0|(27,[0],[1.0])|           (4,[0],[1.0])|[0.0,33.39,-5.0]|[0.0,0.0015938531...|(40,[1,2,3,4,5,6,...|0.505940503222551|\n",
      "|      0|        1|   40.23|   SP|                 1|                -2|           1|                   1|         credit_card|               2|             1.0|    1|        0.0|                       0.0|(27,[0],[1.0])|           (4,[0],[1.0])|[0.0,40.23,-2.0]|[0.0,0.0020948620...|(40,[1,2,3,4,5,6,...|0.505940503222551|\n",
      "|      0|        1|   45.29|   SP|                 1|                -7|           1|                   1|         credit_card|               1|             5.0|    1|        0.0|                       0.0|(27,[0],[1.0])|           (4,[0],[1.0])|[0.0,45.29,-7.0]|[0.0,0.0024654915...|(40,[1,2,3,4,5,6,...|0.505940503222551|\n",
      "|      0|        1|   47.51|   SP|                 1|                -6|           1|                   1|         credit_card|               4|             3.0|    1|        0.0|                       0.0|(27,[0],[1.0])|           (4,[0],[1.0])|[0.0,47.51,-6.0]|[0.0,0.0026280997...|(40,[1,2,3,4,5,6,...|0.505940503222551|\n",
      "+-------+---------+--------+-----+------------------+------------------+------------+--------------------+--------------------+----------------+----------------+-----+-----------+--------------------------+--------------+------------------------+----------------+--------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Calculate the weight\n",
    "# ===================================\n",
    "\n",
    "total_count = train_transformed.count()\n",
    "loyal_count = train_transformed.filter(F.col(\"label\") == 0).count()\n",
    "churn_count = train_transformed.filter(F.col(\"label\") == 1).count()\n",
    "\n",
    "weight_for_loyal = total_count / (2.0 * loyal_count)\n",
    "weight_for_churn = total_count / (2.0 * churn_count)\n",
    "\n",
    "# 2. Add a weight column to your dataframe\n",
    "train_with_weights = train_transformed.withColumn(\"weight\", \n",
    "    F.when(F.col(\"label\") == 0, weight_for_loyal).otherwise(weight_for_churn)\n",
    ")\n",
    "train_with_weights.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c775864",
   "metadata": {},
   "source": [
    "### Need to work from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e4b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6edd528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef evaluate_churn_model(predictions, model_name=\"Model\"):\\n    # STEP 1: SETUP EVALUATORS\\n    # binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\")\\n    binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\") # For SVM\\n\\n    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\\n\\n    # STEP 2: CALCULATE GLOBAL METRICS\\n    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\\n    f1_score = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\\n    roc_auc  = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\\n    pr_auc   = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderPR\"})\\n\\n    # STEP 3: CALCULATE CLASS-SPECIFIC METRICS (Label 0 = Loyal)\\n    loyalty_precision = multi_evaluator.evaluate(predictions, {\\n        multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 0.0})\\n    loyalty_recall = multi_evaluator.evaluate(predictions, {\\n        multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 0.0})\\n\\n    # STEP 4. DISPLAY RESULTS\\n    print(\"\\n\" + \"=\"*50)\\n    print(f\"      PERFORMANCE REPORT: {model_name.upper()}      \")\\n    print(\"=\"*50)\\n    print(f\"{\\'Overall Accuracy:\\':<25} {accuracy:.4f}\")\\n    print(f\"{\\'F1-Score:\\':<25} {f1_score:.4f}\")\\n    print(\"-\" * 50)\\n    print(f\"{\\'ROC AUC:\\':<25} {roc_auc:.4f} (Separation Power)\")\\n    print(f\"{\\'PR AUC:\\':<25} {pr_auc:.4f} (Loyalty Focus)\")\\n    print(\"-\" * 50)\\n    print(f\"{\\'LOYALTY Precision:\\':<25} {loyalty_precision:.4f} (Accuracy of \\'Loyal\\' flags)\")\\n    print(f\"{\\'LOYALTY Recall:\\':<25} {loyalty_recall:.4f} (Coverage of actual Loyalists)\")\\n    print(\"-\" * 50)\\n\\n    # 5. CONFUSION MATRIX\\n    print(\"\\nConfusion Matrix (Label 0=Loyal, 1=Churn):\")\\n    predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\\n\\n    # 6. BUSINESS SUMMARY\\n    tp = predictions.filter(\"label = 0 AND prediction = 0.0\").count()\\n    fn = predictions.filter(\"label = 0 AND prediction = 1.0\").count()\\n    total_loyal = tp + fn\\n    catch_rate = (tp / total_loyal * 100) if total_loyal > 0 else 0\\n\\n    print(\"Business Summary:\")\\n    print(f\">> Model caught {tp} out of {total_loyal} loyal customers ({catch_rate:.1f}%).\")\\n    print(f\">> Model missed {fn} loyal customers (high-risk errors).\")\\n    print(\"=\"*50 + \"\\n\")\\n\\n    # Return metrics as a dict in case you want to store them for a final table\\n    return {\"model\": model_name, \"auc\": roc_auc, \"pr_auc\": pr_auc, \"recall_0\": loyalty_recall}\\n'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================\n",
    "# Evaluates a churn model's predictions\n",
    "# =========================================\n",
    "'''\n",
    "def evaluate_churn_model(predictions, model_name=\"Model\"):\n",
    "    # STEP 1: SETUP EVALUATORS\n",
    "    # binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\")\n",
    "    binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\",rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\") # For SVM\n",
    "\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    # STEP 2: CALCULATE GLOBAL METRICS\n",
    "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    f1_score = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "    roc_auc  = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "    pr_auc   = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    # STEP 3: CALCULATE CLASS-SPECIFIC METRICS (Label 0 = Loyal)\n",
    "    loyalty_precision = multi_evaluator.evaluate(predictions, {\n",
    "        multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "    loyalty_recall = multi_evaluator.evaluate(predictions, {\n",
    "        multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "\n",
    "    # STEP 4. DISPLAY RESULTS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"      PERFORMANCE REPORT: {model_name.upper()}      \")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Overall Accuracy:':<25} {accuracy:.4f}\")\n",
    "    print(f\"{'F1-Score:':<25} {f1_score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'ROC AUC:':<25} {roc_auc:.4f} (Separation Power)\")\n",
    "    print(f\"{'PR AUC:':<25} {pr_auc:.4f} (Loyalty Focus)\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'LOYALTY Precision:':<25} {loyalty_precision:.4f} (Accuracy of 'Loyal' flags)\")\n",
    "    print(f\"{'LOYALTY Recall:':<25} {loyalty_recall:.4f} (Coverage of actual Loyalists)\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # 5. CONFUSION MATRIX\n",
    "    print(\"\\nConfusion Matrix (Label 0=Loyal, 1=Churn):\")\n",
    "    predictions.groupBy(\"label\", \"prediction\").count().orderBy(\"label\", \"prediction\").show()\n",
    "\n",
    "    # 6. BUSINESS SUMMARY\n",
    "    tp = predictions.filter(\"label = 0 AND prediction = 0.0\").count()\n",
    "    fn = predictions.filter(\"label = 0 AND prediction = 1.0\").count()\n",
    "    total_loyal = tp + fn\n",
    "    catch_rate = (tp / total_loyal * 100) if total_loyal > 0 else 0\n",
    "    \n",
    "    print(\"Business Summary:\")\n",
    "    print(f\">> Model caught {tp} out of {total_loyal} loyal customers ({catch_rate:.1f}%).\")\n",
    "    print(f\">> Model missed {fn} loyal customers (high-risk errors).\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Return metrics as a dict in case you want to store them for a final table\n",
    "    return {\"model\": model_name, \"auc\": roc_auc, \"pr_auc\": pr_auc, \"recall_0\": loyalty_recall}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6f84e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================\n",
    "# Evaluates a churn model's predictions\n",
    "# =========================================\n",
    "def evaluate_churn_model(predictions, model_name=\"Model\"):\n",
    "   \n",
    "    # 1. SETUP EVALUATORS\n",
    "    # We use rawPrediction for AUC/PR calculations (works for both Probabilistic models and SVM)\n",
    "    binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\")\n",
    "    multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "\n",
    "    # 2. CALCULATE ALL METRICS\n",
    "    accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "    f1_score = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "    roc_auc  = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderROC\"})\n",
    "    pr_auc   = binary_evaluator.evaluate(predictions, {binary_evaluator.metricName: \"areaUnderPR\"})\n",
    "\n",
    "    # Class-specific metrics for Loyalty (Label 0)\n",
    "    loyalty_precision = multi_evaluator.evaluate(predictions, {\n",
    "        multi_evaluator.metricName: \"precisionByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "    loyalty_recall = multi_evaluator.evaluate(predictions, {\n",
    "        multi_evaluator.metricName: \"recallByLabel\", multi_evaluator.metricLabel: 0.0})\n",
    "\n",
    "    # 3. CALCULATE BUSINESS SUMMARY DATA\n",
    "    # tp = True Positives (Predicted Loyal, Actually Loyal)\n",
    "    # fn = False Negatives (Predicted Churn, Actually Loyal)\n",
    "    tp = predictions.filter(\"label = 0 AND prediction = 0.0\").count()\n",
    "    fn = predictions.filter(\"label = 0 AND prediction = 1.0\").count()\n",
    "    \n",
    "    total_loyal = tp + fn\n",
    "    catch_rate = (tp / total_loyal * 100) if total_loyal > 0 else 0\n",
    "    \n",
    "    # 4. DISPLAY ONLY BUSINESS SUMMARY\n",
    "    print(f\"\\n>>> BUSINESS SUMMARY: {model_name.upper()}\")\n",
    "    print(f\">> Model caught {tp} out of {total_loyal} loyal customers ({catch_rate:.1f}%).\")\n",
    "    print(f\">> Model missed {fn} loyal customers (high-risk errors).\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # 5. RETURN COMPLETE DICTIONARY\n",
    "    # This dictionary will be used to build your final leaderboard DataFrame\n",
    "    return {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1_score,\n",
    "        \"auc\": roc_auc,\n",
    "        \"pr_auc\": pr_auc,\n",
    "        \"loyalty_precision\": loyalty_precision,\n",
    "        \"loyalty_recall\": loyalty_recall,\n",
    "        \"caught_loyal\": tp,\n",
    "        \"missed_loyal\": fn\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32836365",
   "metadata": {},
   "source": [
    "### Train Model\n",
    "**Train Model using all of the Selected Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5c68ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and delete Negative Values\n",
    "def has_negative(vector):\n",
    "    if vector is not None:\n",
    "        return any(x < 0 for x in vector.toArray())\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "004f763a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION\n",
      ">> Model caught 71 out of 134 loyal customers (53.0%).\n",
      ">> Model missed 63 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST\n",
      ">> Model caught 51 out of 134 loyal customers (38.1%).\n",
      ">> Model missed 83 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT\n",
      ">> Model caught 54 out of 134 loyal customers (40.3%).\n",
      ">> Model missed 80 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC\n",
      ">> Model caught 67 out of 134 loyal customers (50.0%).\n",
      ">> Model missed 67 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES\n",
      ">> Model caught 77 out of 134 loyal customers (57.5%).\n",
      ">> Model missed 57 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "--- FINAL BEST EVALUATION LEADERBOARD ---\n",
      "                 model  accuracy        f1       auc    pr_auc  \\\n",
      "0  Logistic Regression  0.590223  0.731680  0.603106  0.991028   \n",
      "3           Linear SVC  0.552211  0.701034  0.581361  0.990621   \n",
      "1        Random Forest  0.674515  0.795196  0.572766  0.989668   \n",
      "2                  GBT  0.668045  0.790537  0.561163  0.989318   \n",
      "5          Naive Bayes  0.490249  0.647169  0.530949  0.988361   \n",
      "4        Decision Tree  0.458034  0.617081  0.508586  0.986785   \n",
      "\n",
      "   loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \n",
      "0           0.015543        0.529851            71            63  \n",
      "3           0.013446        0.500000            67            67  \n",
      "1           0.014206        0.380597            51            83  \n",
      "2           0.014722        0.402985            54            80  \n",
      "5           0.013528        0.574627            77            57  \n",
      "4           0.014330        0.649254            87            47  \n"
     ]
    }
   ],
   "source": [
    "# Define the \"Model Zoo\"\n",
    "classifiers = [\n",
    "    (\"Logistic Regression\", LogisticRegression(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "    (\"Random Forest\", RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "    (\"GBT\", GBTClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "    (\"Linear SVC\", LinearSVC(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "    (\"Naive Bayes\", NaiveBayes(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\"))    \n",
    "]\n",
    "\n",
    "results_leaderboard = []\n",
    "\n",
    "for name, clf in classifiers:\n",
    "    print(f\"Training {name}...\")\n",
    "    model = clf.fit(train_with_weights)\n",
    "\n",
    "    if name == \"Naive Bayes\":\n",
    "        check_neg_udf = F.udf(has_negative, BooleanType())\n",
    "        test_transformed_cleaned = test_transformed.filter(~check_neg_udf(F.col(\"features\")))\n",
    "        predictions = model.transform(test_transformed_cleaned)\n",
    "    else:\n",
    "        predictions = model.transform(test_transformed)\n",
    "    \n",
    "    metrics = evaluate_churn_model(predictions, name)\n",
    "    results_leaderboard.append(metrics)\n",
    "\n",
    "# SHOW FINAL COMPARISON TABLE\n",
    "leaderboard_df = pd.DataFrame(results_leaderboard)\n",
    "print(\"\\n--- FINAL BEST EVALUATION LEADERBOARD ---\")\n",
    "print(leaderboard_df.sort_values(by=\"pr_auc\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b4f7633a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your experiment results have been saved to 'churn_model_experiments_final.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>loyalty_precision</th>\n",
       "      <th>loyalty_recall</th>\n",
       "      <th>caught_loyal</th>\n",
       "      <th>missed_loyal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.590223</td>\n",
       "      <td>0.731680</td>\n",
       "      <td>0.603106</td>\n",
       "      <td>0.991028</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>0.529851</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.552211</td>\n",
       "      <td>0.701034</td>\n",
       "      <td>0.581361</td>\n",
       "      <td>0.990621</td>\n",
       "      <td>0.013446</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.674515</td>\n",
       "      <td>0.795196</td>\n",
       "      <td>0.572766</td>\n",
       "      <td>0.989668</td>\n",
       "      <td>0.014206</td>\n",
       "      <td>0.380597</td>\n",
       "      <td>51</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GBT</td>\n",
       "      <td>0.668045</td>\n",
       "      <td>0.790537</td>\n",
       "      <td>0.561163</td>\n",
       "      <td>0.989318</td>\n",
       "      <td>0.014722</td>\n",
       "      <td>0.402985</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.490249</td>\n",
       "      <td>0.647169</td>\n",
       "      <td>0.530949</td>\n",
       "      <td>0.988361</td>\n",
       "      <td>0.013528</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy        f1       auc    pr_auc  \\\n",
       "0  Logistic Regression  0.590223  0.731680  0.603106  0.991028   \n",
       "3           Linear SVC  0.552211  0.701034  0.581361  0.990621   \n",
       "1        Random Forest  0.674515  0.795196  0.572766  0.989668   \n",
       "2                  GBT  0.668045  0.790537  0.561163  0.989318   \n",
       "5          Naive Bayes  0.490249  0.647169  0.530949  0.988361   \n",
       "\n",
       "   loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \n",
       "0           0.015543        0.529851            71            63  \n",
       "3           0.013446        0.500000            67            67  \n",
       "1           0.014206        0.380597            51            83  \n",
       "2           0.014722        0.402985            54            80  \n",
       "5           0.013528        0.574627            77            57  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_leaderboard = pd.DataFrame(leaderboard_df)\n",
    "\n",
    "final_leaderboard = final_leaderboard.sort_values(by=\"pr_auc\", ascending=False)\n",
    "\n",
    "# 3. Save to CSV\n",
    "final_leaderboard.to_csv(\"Result_Data/1.Default_para_All_features.csv\", index=False)\n",
    "\n",
    "print(\"Success! Your experiment results have been saved to 'churn_model_experiments_final.csv'\")\n",
    "\n",
    "display(final_leaderboard.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cb807",
   "metadata": {},
   "source": [
    "**Feature Importance Ranking Based  Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1b6758cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TOP RANKED FEATURES ---\n",
      "1. recency             : 0.1843\n",
      "2. total_items_volume  : 0.1699\n",
      "3. frequency           : 0.1342\n",
      "4. monetary            : 0.0951\n",
      "5. avg_satisfaction    : 0.0851\n",
      "6. max_delivery_delay  : 0.0811\n",
      "7. state_0             : 0.0474\n",
      "8. max_installments    : 0.0433\n",
      "9. state_1             : 0.0174\n",
      "10. state_11            : 0.0169\n",
      "11. primary_payment_type_0: 0.0113\n",
      "12. state_5             : 0.0098\n",
      "13. state_13            : 0.0098\n",
      "14. primary_payment_type_1: 0.0094\n",
      "15. state_9             : 0.0091\n",
      "16. state_7             : 0.0083\n",
      "17. payment_method_count: 0.0078\n",
      "18. is_delivered        : 0.0072\n",
      "19. primary_payment_type_3: 0.0068\n",
      "20. state_10            : 0.0068\n",
      "21. state_6             : 0.0067\n",
      "22. state_15            : 0.0044\n",
      "23. primary_payment_type_2: 0.0044\n",
      "24. state_4             : 0.0043\n",
      "25. state_8             : 0.0039\n",
      "26. state_14            : 0.0028\n",
      "27. state_2             : 0.0024\n",
      "28. state_19            : 0.0021\n",
      "29. state_12            : 0.0019\n",
      "30. state_3             : 0.0019\n",
      "31. state_17            : 0.0017\n",
      "32. state_18            : 0.0011\n",
      "33. state_22            : 0.0010\n",
      "34. state_16            : 0.0002\n",
      "35. state_20            : 0.0000\n",
      "36. state_21            : 0.0000\n",
      "37. state_23            : 0.0000\n",
      "38. state_24            : 0.0000\n",
      "39. state_25            : 0.0000\n",
      "40. state_26            : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# =====================================\n",
    "# Feature Importance Ranking\n",
    "# =====================================\n",
    "\n",
    "# STEP 1: GET ALL FEATURE NAMES IN ORDER\n",
    "all_feature_names = []\n",
    "for col in norm_cols: all_feature_names.append(col)\n",
    "for col in other_numeric_cols: all_feature_names.append(col)\n",
    "for col in onehot_cols:\n",
    "    num_cats = train_with_weights.select(f\"{col}_ohe\").first()[0].size\n",
    "    for i in range(num_cats):\n",
    "        all_feature_names.append(f\"{col}_{i}\")\n",
    "\n",
    "# STEP 2. GET RANKINGS\n",
    "rf_baseline = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", weightCol=\"weight\", seed=42)\n",
    "rf_model_baseline = rf_baseline.fit(train_with_weights)\n",
    "\n",
    "# 3. MAP AND SORT IMPORTANCE\n",
    "importances = rf_model_baseline.featureImportances.toArray()\n",
    "feature_importance_map = sorted(zip(all_feature_names, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"--- TOP RANKED FEATURES ---\")\n",
    "for i, (name, score) in enumerate(feature_importance_map[:]):\n",
    "    print(f\"{i+1}. {name:<20}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85e555c",
   "metadata": {},
   "source": [
    "**Train The Model using top 3 to 15 rank Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9a19209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 3 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-3\n",
      ">> Model caught 67 out of 134 loyal customers (50.0%).\n",
      ">> Model missed 67 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-3\n",
      ">> Model caught 62 out of 134 loyal customers (46.3%).\n",
      ">> Model missed 72 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-3\n",
      ">> Model caught 61 out of 134 loyal customers (45.5%).\n",
      ">> Model missed 73 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-3\n",
      ">> Model caught 19 out of 134 loyal customers (14.2%).\n",
      ">> Model missed 115 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-3\n",
      ">> Model caught 62 out of 134 loyal customers (46.3%).\n",
      ">> Model missed 72 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-3\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 4 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-4\n",
      ">> Model caught 66 out of 134 loyal customers (49.3%).\n",
      ">> Model missed 68 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-4\n",
      ">> Model caught 60 out of 134 loyal customers (44.8%).\n",
      ">> Model missed 74 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-4\n",
      ">> Model caught 60 out of 134 loyal customers (44.8%).\n",
      ">> Model missed 74 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-4\n",
      ">> Model caught 19 out of 134 loyal customers (14.2%).\n",
      ">> Model missed 115 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-4\n",
      ">> Model caught 63 out of 134 loyal customers (47.0%).\n",
      ">> Model missed 71 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-4\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 5 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-5\n",
      ">> Model caught 74 out of 134 loyal customers (55.2%).\n",
      ">> Model missed 60 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-5\n",
      ">> Model caught 51 out of 134 loyal customers (38.1%).\n",
      ">> Model missed 83 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-5\n",
      ">> Model caught 67 out of 134 loyal customers (50.0%).\n",
      ">> Model missed 67 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-5\n",
      ">> Model caught 54 out of 134 loyal customers (40.3%).\n",
      ">> Model missed 80 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-5\n",
      ">> Model caught 98 out of 134 loyal customers (73.1%).\n",
      ">> Model missed 36 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-5\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 6 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-6\n",
      ">> Model caught 73 out of 134 loyal customers (54.5%).\n",
      ">> Model missed 61 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-6\n",
      ">> Model caught 43 out of 134 loyal customers (32.1%).\n",
      ">> Model missed 91 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-6\n",
      ">> Model caught 63 out of 134 loyal customers (47.0%).\n",
      ">> Model missed 71 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-6\n",
      ">> Model caught 68 out of 134 loyal customers (50.7%).\n",
      ">> Model missed 66 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-6\n",
      ">> Model caught 95 out of 134 loyal customers (70.9%).\n",
      ">> Model missed 39 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-6\n",
      ">> Model caught 56 out of 134 loyal customers (41.8%).\n",
      ">> Model missed 78 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 7 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-7...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-7\n",
      ">> Model caught 66 out of 134 loyal customers (49.3%).\n",
      ">> Model missed 68 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-7...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-7\n",
      ">> Model caught 57 out of 134 loyal customers (42.5%).\n",
      ">> Model missed 77 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-7...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-7\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-7...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-7\n",
      ">> Model caught 63 out of 134 loyal customers (47.0%).\n",
      ">> Model missed 71 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-7...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-7\n",
      ">> Model caught 79 out of 134 loyal customers (59.0%).\n",
      ">> Model missed 55 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-7...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-7\n",
      ">> Model caught 63 out of 134 loyal customers (47.0%).\n",
      ">> Model missed 71 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 8 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-8...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-8\n",
      ">> Model caught 66 out of 134 loyal customers (49.3%).\n",
      ">> Model missed 68 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-8...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-8\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-8...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-8\n",
      ">> Model caught 52 out of 134 loyal customers (38.8%).\n",
      ">> Model missed 82 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-8...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-8\n",
      ">> Model caught 63 out of 134 loyal customers (47.0%).\n",
      ">> Model missed 71 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-8...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-8\n",
      ">> Model caught 81 out of 134 loyal customers (60.4%).\n",
      ">> Model missed 53 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-8...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-8\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 9 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-9...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-9\n",
      ">> Model caught 69 out of 134 loyal customers (51.5%).\n",
      ">> Model missed 65 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-9...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-9\n",
      ">> Model caught 55 out of 134 loyal customers (41.0%).\n",
      ">> Model missed 79 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-9...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-9\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-9...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-9\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-9...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-9\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-9...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-9\n",
      ">> Model caught 65 out of 134 loyal customers (48.5%).\n",
      ">> Model missed 69 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 10 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-10...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-10\n",
      ">> Model caught 70 out of 134 loyal customers (52.2%).\n",
      ">> Model missed 64 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-10...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-10\n",
      ">> Model caught 56 out of 134 loyal customers (41.8%).\n",
      ">> Model missed 78 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-10...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-10\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-10...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-10\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-10...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-10\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-10...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-10\n",
      ">> Model caught 72 out of 134 loyal customers (53.7%).\n",
      ">> Model missed 62 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 11 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-11...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-11\n",
      ">> Model caught 71 out of 134 loyal customers (53.0%).\n",
      ">> Model missed 63 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-11...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-11\n",
      ">> Model caught 56 out of 134 loyal customers (41.8%).\n",
      ">> Model missed 78 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-11...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-11\n",
      ">> Model caught 51 out of 134 loyal customers (38.1%).\n",
      ">> Model missed 83 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-11...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-11\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-11...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-11\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-11...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-11\n",
      ">> Model caught 71 out of 134 loyal customers (53.0%).\n",
      ">> Model missed 63 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 12 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-12\n",
      ">> Model caught 71 out of 134 loyal customers (53.0%).\n",
      ">> Model missed 63 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-12\n",
      ">> Model caught 56 out of 134 loyal customers (41.8%).\n",
      ">> Model missed 78 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-12\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-12\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-12\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-12\n",
      ">> Model caught 74 out of 134 loyal customers (55.2%).\n",
      ">> Model missed 60 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 13 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-13...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-13\n",
      ">> Model caught 70 out of 134 loyal customers (52.2%).\n",
      ">> Model missed 64 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-13...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-13\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-13...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-13\n",
      ">> Model caught 50 out of 134 loyal customers (37.3%).\n",
      ">> Model missed 84 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-13...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-13\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-13...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-13\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-13...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-13\n",
      ">> Model caught 74 out of 134 loyal customers (55.2%).\n",
      ">> Model missed 60 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 14 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-14...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-14\n",
      ">> Model caught 70 out of 134 loyal customers (52.2%).\n",
      ">> Model missed 64 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-14...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-14\n",
      ">> Model caught 53 out of 134 loyal customers (39.6%).\n",
      ">> Model missed 81 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training GBT Top-14...\n",
      "\n",
      ">>> BUSINESS SUMMARY: GBT TOP-14\n",
      ">> Model caught 52 out of 134 loyal customers (38.8%).\n",
      ">> Model missed 82 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-14...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-14\n",
      ">> Model caught 63 out of 134 loyal customers (47.0%).\n",
      ">> Model missed 71 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Decision Tree Top-14...\n",
      "\n",
      ">>> BUSINESS SUMMARY: DECISION TREE TOP-14\n",
      ">> Model caught 87 out of 134 loyal customers (64.9%).\n",
      ">> Model missed 47 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Naive Bayes Top-14...\n",
      "\n",
      ">>> BUSINESS SUMMARY: NAIVE BAYES TOP-14\n",
      ">> Model caught 77 out of 134 loyal customers (57.5%).\n",
      ">> Model missed 57 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "--- FEATURE IMPORTANCE EXPERIMENT SUMMARY ---\n",
      "                        model  accuracy        f1       auc    pr_auc  \\\n",
      "6   Logistic Regression Top-4  0.663731  0.787251  0.627279  0.992633   \n",
      "9            Linear SVC Top-4  0.937994  0.956922  0.629204  0.992489   \n",
      "7         Random Forest Top-4  0.743979  0.842508  0.629268  0.992376   \n",
      "3            Linear SVC Top-3  0.937994  0.956922  0.627644  0.992104   \n",
      "0   Logistic Regression Top-3  0.681165  0.799677  0.627857  0.992093   \n",
      "..                        ...       ...       ...       ...       ...   \n",
      "17          Naive Bayes Top-5  0.691831  0.807342  0.485256  0.986700   \n",
      "23          Naive Bayes Top-6  0.695875  0.810113  0.483280  0.986624   \n",
      "71         Naive Bayes Top-14  0.554507  0.702646  0.516279  0.986387   \n",
      "11          Naive Bayes Top-4  0.520805  0.673805  0.464784  0.985316   \n",
      "5           Naive Bayes Top-3  0.520219  0.673299  0.467833  0.985184   \n",
      "\n",
      "    loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \n",
      "6            0.017647        0.492537            66            68  \n",
      "9            0.031987        0.141791            19           115  \n",
      "7            0.021164        0.447761            60            74  \n",
      "3            0.031987        0.141791            19           115  \n",
      "0            0.018884        0.500000            67            67  \n",
      "..                ...             ...           ...           ...  \n",
      "17           0.015584        0.395522            53            81  \n",
      "23           0.016657        0.417910            56            78  \n",
      "71           0.015471        0.574627            77            57  \n",
      "11           0.016195        0.649254            87            47  \n",
      "5            0.016174        0.649254            87            47  \n",
      "\n",
      "[72 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = []\n",
    "\n",
    "# Check and delete Negative Values\n",
    "def has_negative(vector):\n",
    "    if vector is not None:\n",
    "        return any(x < 0 for x in vector.toArray())\n",
    "    return False\n",
    "\n",
    "\n",
    "for k in range(3,15):\n",
    "    print(50 * \"=\")\n",
    "    print(f\"\\n>>> Running Experiment: Top {k} Features\")\n",
    "    print(50 * \"=\")\n",
    "    \n",
    "    top_features = [f[0] for f in feature_importance_map[:k]] # top K features names\n",
    "    top_indices = [all_feature_names.index(name) for name in top_features]\n",
    "    \n",
    "    # Note: For strict \"Top K\", we would slice the existing 'features' vector\n",
    "    from pyspark.ml.feature import VectorSlicer\n",
    "    slicer = VectorSlicer(inputCol=\"features\", outputCol=\"top_k_features\", indices=top_indices)\n",
    "    \n",
    "    train_subset = slicer.transform(train_with_weights)\n",
    "    test_subset = slicer.transform(test_transformed)\n",
    "\n",
    "    # \"Model Zoo\"\n",
    "    classifiers = [\n",
    "        (\"Logistic Regression\", LogisticRegression(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Random Forest\", RandomForestClassifier(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"GBT\", GBTClassifier(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Linear SVC\", LinearSVC(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Decision Tree\", DecisionTreeClassifier(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Naive Bayes\", NaiveBayes(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\"))\n",
    "    ]\n",
    "\n",
    "    results_leaderboard = []\n",
    "\n",
    "    for name, clf in classifiers:\n",
    "        print(f\"Training {name} Top-{k}...\")\n",
    "        model = clf.fit(train_subset)\n",
    "\n",
    "        if name == \"Naive Bayes\":\n",
    "            check_neg_udf = F.udf(has_negative, BooleanType())\n",
    "            test_transformed_cleaned = test_subset.filter(~check_neg_udf(F.col(\"top_k_features\")))\n",
    "            predictions = model.transform(test_transformed_cleaned)\n",
    "        else:\n",
    "            predictions = model.transform(test_subset)\n",
    "        \n",
    "        metrics = evaluate_churn_model(predictions, f\"{name} Top-{k}\")\n",
    "        experiment_results.append(metrics)\n",
    "    \n",
    "# 3. FINAL COMPARISON TABLE\n",
    "comparison_df = pd.DataFrame(experiment_results)\n",
    "print(\"\\n--- FEATURE IMPORTANCE EXPERIMENT SUMMARY ---\")\n",
    "print(comparison_df.sort_values(by=\"pr_auc\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4d333911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your experiment results have been saved to 'churn_model_experiments_final.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>loyalty_precision</th>\n",
       "      <th>loyalty_recall</th>\n",
       "      <th>caught_loyal</th>\n",
       "      <th>missed_loyal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression Top-4</td>\n",
       "      <td>0.663731</td>\n",
       "      <td>0.787251</td>\n",
       "      <td>0.627279</td>\n",
       "      <td>0.992633</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Linear SVC Top-4</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.956922</td>\n",
       "      <td>0.629204</td>\n",
       "      <td>0.992489</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest Top-4</td>\n",
       "      <td>0.743979</td>\n",
       "      <td>0.842508</td>\n",
       "      <td>0.629268</td>\n",
       "      <td>0.992376</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>60</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Linear SVC Top-3</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.956922</td>\n",
       "      <td>0.627644</td>\n",
       "      <td>0.992104</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression Top-3</td>\n",
       "      <td>0.681165</td>\n",
       "      <td>0.799677</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.992093</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression Top-5</td>\n",
       "      <td>0.626348</td>\n",
       "      <td>0.759518</td>\n",
       "      <td>0.621696</td>\n",
       "      <td>0.991993</td>\n",
       "      <td>0.017737</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Logistic Regression Top-6</td>\n",
       "      <td>0.628145</td>\n",
       "      <td>0.760894</td>\n",
       "      <td>0.616994</td>\n",
       "      <td>0.991902</td>\n",
       "      <td>0.017590</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>73</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Linear SVC Top-6</td>\n",
       "      <td>0.677301</td>\n",
       "      <td>0.796927</td>\n",
       "      <td>0.623519</td>\n",
       "      <td>0.991887</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Random Forest Top-5</td>\n",
       "      <td>0.757189</td>\n",
       "      <td>0.851163</td>\n",
       "      <td>0.615763</td>\n",
       "      <td>0.991858</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.380597</td>\n",
       "      <td>51</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Logistic Regression Top-12</td>\n",
       "      <td>0.620417</td>\n",
       "      <td>0.755084</td>\n",
       "      <td>0.613511</td>\n",
       "      <td>0.991829</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.529851</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         model  accuracy        f1       auc    pr_auc  \\\n",
       "6    Logistic Regression Top-4  0.663731  0.787251  0.627279  0.992633   \n",
       "9             Linear SVC Top-4  0.937994  0.956922  0.629204  0.992489   \n",
       "7          Random Forest Top-4  0.743979  0.842508  0.629268  0.992376   \n",
       "3             Linear SVC Top-3  0.937994  0.956922  0.627644  0.992104   \n",
       "0    Logistic Regression Top-3  0.681165  0.799677  0.627857  0.992093   \n",
       "12   Logistic Regression Top-5  0.626348  0.759518  0.621696  0.991993   \n",
       "18   Logistic Regression Top-6  0.628145  0.760894  0.616994  0.991902   \n",
       "21            Linear SVC Top-6  0.677301  0.796927  0.623519  0.991887   \n",
       "13         Random Forest Top-5  0.757189  0.851163  0.615763  0.991858   \n",
       "54  Logistic Regression Top-12  0.620417  0.755084  0.613511  0.991829   \n",
       "\n",
       "    loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \n",
       "6            0.017647        0.492537            66            68  \n",
       "9            0.031987        0.141791            19           115  \n",
       "7            0.021164        0.447761            60            74  \n",
       "3            0.031987        0.141791            19           115  \n",
       "0            0.018884        0.500000            67            67  \n",
       "12           0.017737        0.552239            74            60  \n",
       "18           0.017590        0.544776            73            61  \n",
       "21           0.018926        0.507463            68            66  \n",
       "13           0.019101        0.380597            51            83  \n",
       "54           0.016777        0.529851            71            63  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_leaderboard = pd.DataFrame(experiment_results)\n",
    "final_leaderboard = final_leaderboard.sort_values(by=\"pr_auc\", ascending=False)\n",
    "\n",
    "# 3. Save to CSV\n",
    "final_leaderboard.to_csv(\"Result_Data/2.Default_para_top_k_features.csv\", index=False)\n",
    "\n",
    "print(\"Success! Your experiment results have been saved to 'churn_model_experiments_final.csv'\")\n",
    "\n",
    "display(final_leaderboard.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b110c2f",
   "metadata": {},
   "source": [
    "**Best Features Combination[3, 4, 5, 6, 12]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e9097928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 3 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-3\n",
      ">> Model caught 67 out of 134 loyal customers (50.0%).\n",
      ">> Model missed 67 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-3\n",
      ">> Model caught 62 out of 134 loyal customers (46.3%).\n",
      ">> Model missed 72 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-3...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-3\n",
      ">> Model caught 19 out of 134 loyal customers (14.2%).\n",
      ">> Model missed 115 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 4 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-4\n",
      ">> Model caught 66 out of 134 loyal customers (49.3%).\n",
      ">> Model missed 68 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-4\n",
      ">> Model caught 60 out of 134 loyal customers (44.8%).\n",
      ">> Model missed 74 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-4...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-4\n",
      ">> Model caught 19 out of 134 loyal customers (14.2%).\n",
      ">> Model missed 115 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 5 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-5\n",
      ">> Model caught 74 out of 134 loyal customers (55.2%).\n",
      ">> Model missed 60 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-5\n",
      ">> Model caught 51 out of 134 loyal customers (38.1%).\n",
      ">> Model missed 83 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-5...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-5\n",
      ">> Model caught 54 out of 134 loyal customers (40.3%).\n",
      ">> Model missed 80 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 6 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-6\n",
      ">> Model caught 73 out of 134 loyal customers (54.5%).\n",
      ">> Model missed 61 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-6\n",
      ">> Model caught 43 out of 134 loyal customers (32.1%).\n",
      ">> Model missed 91 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-6...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-6\n",
      ">> Model caught 68 out of 134 loyal customers (50.7%).\n",
      ">> Model missed 66 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      ">>> Running Experiment: Top 12 Features\n",
      "==================================================\n",
      "Training Logistic Regression Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION TOP-12\n",
      ">> Model caught 71 out of 134 loyal customers (53.0%).\n",
      ">> Model missed 63 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Random Forest Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST TOP-12\n",
      ">> Model caught 56 out of 134 loyal customers (41.8%).\n",
      ">> Model missed 78 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "Training Linear SVC Top-12...\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC TOP-12\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "--- FEATURE IMPORTANCE EXPERIMENT SUMMARY ---\n",
      "                         model  accuracy        f1       auc    pr_auc  \\\n",
      "3    Logistic Regression Top-4  0.663731  0.787251  0.627279  0.992633   \n",
      "5             Linear SVC Top-4  0.937994  0.956922  0.629204  0.992489   \n",
      "4          Random Forest Top-4  0.743979  0.842508  0.629268  0.992376   \n",
      "2             Linear SVC Top-3  0.937994  0.956922  0.627644  0.992104   \n",
      "0    Logistic Regression Top-3  0.681165  0.799677  0.627857  0.992093   \n",
      "6    Logistic Regression Top-5  0.626348  0.759518  0.621696  0.991993   \n",
      "9    Logistic Regression Top-6  0.628145  0.760894  0.616994  0.991902   \n",
      "11            Linear SVC Top-6  0.677301  0.796927  0.623519  0.991887   \n",
      "7          Random Forest Top-5  0.757189  0.851163  0.615763  0.991858   \n",
      "12  Logistic Regression Top-12  0.620417  0.755084  0.613511  0.991829   \n",
      "8             Linear SVC Top-5  0.767074  0.857494  0.626325  0.991825   \n",
      "1          Random Forest Top-3  0.725198  0.830035  0.618494  0.991492   \n",
      "14           Linear SVC Top-12  0.582135  0.725436  0.588225  0.991305   \n",
      "13        Random Forest Top-12  0.687635  0.804375  0.578257  0.990483   \n",
      "10         Random Forest Top-6  0.726725  0.831231  0.574844  0.990451   \n",
      "\n",
      "    loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \n",
      "3            0.017647        0.492537            66            68  \n",
      "5            0.031987        0.141791            19           115  \n",
      "4            0.021164        0.447761            60            74  \n",
      "2            0.031987        0.141791            19           115  \n",
      "0            0.018884        0.500000            67            67  \n",
      "6            0.017737        0.552239            74            60  \n",
      "9            0.017590        0.544776            73            61  \n",
      "11           0.018926        0.507463            68            66  \n",
      "7            0.019101        0.380597            51            83  \n",
      "12           0.016777        0.529851            71            63  \n",
      "8            0.021044        0.402985            54            80  \n",
      "1            0.020341        0.462687            62            72  \n",
      "14           0.013781        0.477612            64            70  \n",
      "13           0.016213        0.417910            56            78  \n",
      "10           0.014367        0.320896            43            91  \n"
     ]
    }
   ],
   "source": [
    "experiment_results = []\n",
    "top_k_list = [3, 4, 5, 6, 12]\n",
    "\n",
    "# Check and delete Negative Values\n",
    "def has_negative(vector):\n",
    "    if vector is not None:\n",
    "        return any(x < 0 for x in vector.toArray())\n",
    "    return False\n",
    "\n",
    "\n",
    "for k in top_k_list:\n",
    "    print(50 * \"=\")\n",
    "    print(f\"\\n>>> Running Experiment: Top {k} Features\")\n",
    "    print(50 * \"=\")\n",
    "    \n",
    "    top_features = [f[0] for f in feature_importance_map[:k]] # top K features names\n",
    "    top_indices = [all_feature_names.index(name) for name in top_features]\n",
    "    \n",
    "    # Note: For strict \"Top K\", we would slice the existing 'features' vector\n",
    "    from pyspark.ml.feature import VectorSlicer\n",
    "    slicer = VectorSlicer(inputCol=\"features\", outputCol=\"top_k_features\", indices=top_indices)\n",
    "    \n",
    "    train_subset = slicer.transform(train_with_weights)\n",
    "    test_subset = slicer.transform(test_transformed)\n",
    "\n",
    "    # Define the \"Model Zoo\"\n",
    "    classifiers = [\n",
    "        (\"Logistic Regression\", LogisticRegression(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Random Forest\", RandomForestClassifier(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Linear SVC\", LinearSVC(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\"))\n",
    "    ]\n",
    "\n",
    "    results_leaderboard = []\n",
    "\n",
    "    for name, clf in classifiers:\n",
    "        print(f\"Training {name} Top-{k}...\")\n",
    "        model = clf.fit(train_subset)\n",
    "\n",
    "        if name == \"Naive Bayes\":\n",
    "            check_neg_udf = F.udf(has_negative, BooleanType())\n",
    "            test_transformed_cleaned = test_subset.filter(~check_neg_udf(F.col(\"top_k_features\")))\n",
    "            predictions = model.transform(test_transformed_cleaned)\n",
    "        else:\n",
    "            predictions = model.transform(test_subset)\n",
    "        \n",
    "        metrics = evaluate_churn_model(predictions, f\"{name} Top-{k}\")\n",
    "        experiment_results.append(metrics)\n",
    "    \n",
    "# 3. FINAL COMPARISON TABLE\n",
    "comparison_df = pd.DataFrame(experiment_results)\n",
    "print(\"\\n--- FEATURE IMPORTANCE EXPERIMENT SUMMARY ---\")\n",
    "print(comparison_df.sort_values(by=\"pr_auc\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb61fe",
   "metadata": {},
   "source": [
    "**Hyper Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9e475d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " EXPERIMENT: TOP 3 FEATURES (TUNING ENABLED) \n",
      "============================================================\n",
      "--- Tuning Logistic Regression (Top-3 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION (TUNED) TOP-3\n",
      ">> Model caught 67 out of 134 loyal customers (50.0%).\n",
      ">> Model missed 67 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Random Forest (Top-3 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST (TUNED) TOP-3\n",
      ">> Model caught 62 out of 134 loyal customers (46.3%).\n",
      ">> Model missed 72 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Linear SVC (Top-3 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC (TUNED) TOP-3\n",
      ">> Model caught 19 out of 134 loyal customers (14.2%).\n",
      ">> Model missed 115 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      " EXPERIMENT: TOP 4 FEATURES (TUNING ENABLED) \n",
      "============================================================\n",
      "--- Tuning Logistic Regression (Top-4 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION (TUNED) TOP-4\n",
      ">> Model caught 66 out of 134 loyal customers (49.3%).\n",
      ">> Model missed 68 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Random Forest (Top-4 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST (TUNED) TOP-4\n",
      ">> Model caught 60 out of 134 loyal customers (44.8%).\n",
      ">> Model missed 74 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Linear SVC (Top-4 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC (TUNED) TOP-4\n",
      ">> Model caught 19 out of 134 loyal customers (14.2%).\n",
      ">> Model missed 115 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      " EXPERIMENT: TOP 5 FEATURES (TUNING ENABLED) \n",
      "============================================================\n",
      "--- Tuning Logistic Regression (Top-5 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION (TUNED) TOP-5\n",
      ">> Model caught 74 out of 134 loyal customers (55.2%).\n",
      ">> Model missed 60 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Random Forest (Top-5 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST (TUNED) TOP-5\n",
      ">> Model caught 51 out of 134 loyal customers (38.1%).\n",
      ">> Model missed 83 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Linear SVC (Top-5 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC (TUNED) TOP-5\n",
      ">> Model caught 54 out of 134 loyal customers (40.3%).\n",
      ">> Model missed 80 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      " EXPERIMENT: TOP 6 FEATURES (TUNING ENABLED) \n",
      "============================================================\n",
      "--- Tuning Logistic Regression (Top-6 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION (TUNED) TOP-6\n",
      ">> Model caught 73 out of 134 loyal customers (54.5%).\n",
      ">> Model missed 61 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Random Forest (Top-6 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST (TUNED) TOP-6\n",
      ">> Model caught 43 out of 134 loyal customers (32.1%).\n",
      ">> Model missed 91 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Linear SVC (Top-6 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC (TUNED) TOP-6\n",
      ">> Model caught 68 out of 134 loyal customers (50.7%).\n",
      ">> Model missed 66 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      " EXPERIMENT: TOP 12 FEATURES (TUNING ENABLED) \n",
      "============================================================\n",
      "--- Tuning Logistic Regression (Top-12 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LOGISTIC REGRESSION (TUNED) TOP-12\n",
      ">> Model caught 71 out of 134 loyal customers (53.0%).\n",
      ">> Model missed 63 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Random Forest (Top-12 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: RANDOM FOREST (TUNED) TOP-12\n",
      ">> Model caught 56 out of 134 loyal customers (41.8%).\n",
      ">> Model missed 78 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "--- Tuning Linear SVC (Top-12 features) ---\n",
      "\n",
      ">>> BUSINESS SUMMARY: LINEAR SVC (TUNED) TOP-12\n",
      ">> Model caught 64 out of 134 loyal customers (47.8%).\n",
      ">> Model missed 70 loyal customers (high-risk errors).\n",
      "==================================================\n",
      "\n",
      "\n",
      "\n",
      "--- TUNED EXPERIMENT SUMMARY ---\n",
      "                                 model  accuracy        f1       auc  \\\n",
      "3    Logistic Regression (Tuned) Top-4  0.663731  0.787251  0.627279   \n",
      "5             Linear SVC (Tuned) Top-4  0.937994  0.956922  0.629204   \n",
      "4          Random Forest (Tuned) Top-4  0.743979  0.842508  0.629268   \n",
      "2             Linear SVC (Tuned) Top-3  0.937994  0.956922  0.627644   \n",
      "0    Logistic Regression (Tuned) Top-3  0.681165  0.799677  0.627857   \n",
      "6    Logistic Regression (Tuned) Top-5  0.626348  0.759518  0.621696   \n",
      "9    Logistic Regression (Tuned) Top-6  0.628145  0.760894  0.616994   \n",
      "11            Linear SVC (Tuned) Top-6  0.677301  0.796927  0.623519   \n",
      "7          Random Forest (Tuned) Top-5  0.757189  0.851163  0.615763   \n",
      "12  Logistic Regression (Tuned) Top-12  0.620417  0.755084  0.613511   \n",
      "8             Linear SVC (Tuned) Top-5  0.767074  0.857494  0.626325   \n",
      "1          Random Forest (Tuned) Top-3  0.725198  0.830035  0.618494   \n",
      "14           Linear SVC (Tuned) Top-12  0.582135  0.725436  0.588225   \n",
      "13        Random Forest (Tuned) Top-12  0.687635  0.804375  0.578257   \n",
      "10         Random Forest (Tuned) Top-6  0.726725  0.831231  0.574844   \n",
      "\n",
      "      pr_auc  loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \\\n",
      "3   0.992633           0.017647        0.492537            66            68   \n",
      "5   0.992489           0.031987        0.141791            19           115   \n",
      "4   0.992376           0.021164        0.447761            60            74   \n",
      "2   0.992104           0.031987        0.141791            19           115   \n",
      "0   0.992093           0.018884        0.500000            67            67   \n",
      "6   0.991993           0.017737        0.552239            74            60   \n",
      "9   0.991902           0.017590        0.544776            73            61   \n",
      "11  0.991887           0.018926        0.507463            68            66   \n",
      "7   0.991858           0.019101        0.380597            51            83   \n",
      "12  0.991829           0.016777        0.529851            71            63   \n",
      "8   0.991825           0.021044        0.402985            54            80   \n",
      "1   0.991492           0.020341        0.462687            62            72   \n",
      "14  0.991305           0.013781        0.477612            64            70   \n",
      "13  0.990483           0.016213        0.417910            56            78   \n",
      "10  0.990451           0.014367        0.320896            43            91   \n",
      "\n",
      "                                          best_params  \n",
      "3   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
      "5   {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
      "4   {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n",
      "2   {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
      "0   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
      "6   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
      "9   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
      "11  {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
      "7   {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n",
      "12  {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
      "8   {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
      "1   {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n",
      "14  {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
      "13  {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n",
      "10  {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "import pandas as pd\n",
    "\n",
    "# 1. SETUP TUNING EVALUATOR (PR AUC is critical for 98:2 imbalance)\n",
    "tuning_evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    rawPredictionCol=\"rawPrediction\", \n",
    "    metricName=\"areaUnderPR\"\n",
    ")\n",
    "\n",
    "# 2. DEFINE PARAMETER GRIDS (Keeping them small for performance)\n",
    "lr_param_grid = (ParamGridBuilder()\n",
    "    .addGrid(LogisticRegression.regParam, [0.01, 0.1, 1.0])          # Controls the strength of regularization\n",
    "    .addGrid(LogisticRegression.elasticNetParam, [0.0, 0.5, 1.0])   # 0=L2 (Ridge), 1=L1 (Lasso), 0.5=Both\n",
    "    .addGrid(LogisticRegression.maxIter, [10, 50, 100])             # Ensures the algorithm has time to converge\n",
    "    .addGrid(LogisticRegression.threshold, [0.3, 0.4, 0.5])         # Essential for imbalanced churn data\n",
    "    .build())\n",
    "\n",
    "rf_param_grid = (ParamGridBuilder()\n",
    "    .addGrid(RandomForestClassifier.numTrees, [50, 100, 200])       # More trees reduce variance\n",
    "    .addGrid(RandomForestClassifier.maxDepth, [5, 10, 15])          # Deeper trees capture more complex patterns\n",
    "    .addGrid(RandomForestClassifier.featureSubsetStrategy, [\"auto\", \"sqrt\", \"log2\"]) # Variety of feature looks\n",
    "    .addGrid(RandomForestClassifier.minInstancesPerNode, [1, 5, 10]) # Prevents overfitting on noise\n",
    "    .build())\n",
    "\n",
    "lsvc_param_grid = (ParamGridBuilder()\n",
    "    .addGrid(LinearSVC.regParam, [0.01, 0.1, 1.0])                  \n",
    "    .addGrid(LinearSVC.maxIter, [10, 50, 100])                      \n",
    "    .addGrid(LinearSVC.standardization, [True, False])              # Whether to scale the data internally\n",
    "    .addGrid(LinearSVC.tol, [1e-6, 1e-4])                           # Convergence tolerance\n",
    "    .build())\n",
    "\n",
    "# Mapping names to grids\n",
    "grid_map = {\n",
    "    \"Logistic Regression\": lr_param_grid,\n",
    "    \"Random Forest\": rf_param_grid,\n",
    "    \"Linear SVC\": lsvc_param_grid\n",
    "}\n",
    "\n",
    "# 3. THE TUNING LOOP\n",
    "experiment_results = []\n",
    "top_k_list = [3, 4, 5, 6, 12]\n",
    "\n",
    "for k in top_k_list:\n",
    "    print(\"\\n\" + 60 * \"=\")\n",
    "    print(f\" EXPERIMENT: TOP {k} FEATURES (TUNING ENABLED) \")\n",
    "    print(60 * \"=\")\n",
    "    \n",
    "    # Slice features based on your ranking\n",
    "    top_features = [f[0] for f in feature_importance_map[:k]]\n",
    "    top_indices = [all_feature_names.index(name) for name in top_features]\n",
    "    \n",
    "    from pyspark.ml.feature import VectorSlicer\n",
    "    slicer = VectorSlicer(inputCol=\"features\", outputCol=\"top_k_features\", indices=top_indices)\n",
    "    \n",
    "    train_subset = slicer.transform(train_with_weights).cache()\n",
    "    test_subset = slicer.transform(test_transformed).cache()\n",
    "\n",
    "    classifiers = [\n",
    "        (\"Logistic Regression\", LogisticRegression(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Random Forest\", RandomForestClassifier(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\")),\n",
    "        (\"Linear SVC\", LinearSVC(featuresCol=\"top_k_features\", labelCol=\"label\", weightCol=\"weight\"))\n",
    "    ]\n",
    "\n",
    "    for name, clf in classifiers:\n",
    "        print(f\"--- Tuning {name} (Top-{k} features) ---\")\n",
    "        \n",
    "        # Initialize CrossValidator\n",
    "        cv = CrossValidator(\n",
    "            estimator=clf,\n",
    "            estimatorParamMaps=grid_map[name],\n",
    "            evaluator=tuning_evaluator,\n",
    "            numFolds=3,\n",
    "            seed=42,\n",
    "            parallelism=1  # <--- parallelism=4 will train 4 models at the same time if your PC allows it\n",
    "        )\n",
    "\n",
    "        # FIT: This runs all grid combinations and picks the best one\n",
    "        cv_model = cv.fit(train_subset)\n",
    "        best_model = cv_model.bestModel\n",
    "        \n",
    "        # TRANSFORM & EVALUATE\n",
    "        predictions = best_model.transform(test_subset)\n",
    "        metrics = evaluate_churn_model(predictions, f\"{name} (Tuned) Top-{k}\")\n",
    "        \n",
    "        # Save the best params to the results for your project documentation\n",
    "        best_params = best_model.extractParamMap()\n",
    "        #metrics[\"best_params\"] = str({p.name: v for p, v in best_params.items() if p.name in ['regParam', 'elasticNetParam', 'numTrees', 'maxDepth']})\n",
    "        metrics[\"best_params\"] = str({p.name: v for p, v in best_params.items() if p.name in \n",
    "                                    ['regParam', 'elasticNetParam', 'numTrees', 'maxDepth', \n",
    "                                    'threshold', 'maxIter', 'featureSubsetStrategy', 'minInstancesPerNode']})\n",
    "\n",
    "        experiment_results.append(metrics)\n",
    "    train_subset.unpersist()\n",
    "    test_subset.unpersist()\n",
    "\n",
    "# 4. FINAL RESULTS LEADERBOARD\n",
    "comparison_df = pd.DataFrame(experiment_results)\n",
    "print(\"\\n--- TUNED EXPERIMENT SUMMARY ---\")\n",
    "# Showing top results by PR AUC\n",
    "print(comparison_df.sort_values(by=\"pr_auc\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "58745632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Your experiment results have been saved to 'churn_model_experiments_final.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>loyalty_precision</th>\n",
       "      <th>loyalty_recall</th>\n",
       "      <th>caught_loyal</th>\n",
       "      <th>missed_loyal</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression (Tuned) Top-4</td>\n",
       "      <td>0.663731</td>\n",
       "      <td>0.787251</td>\n",
       "      <td>0.627279</td>\n",
       "      <td>0.992633</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.492537</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>{'elasticNetParam': 0.0, 'maxIter': 100, 'regP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Linear SVC (Tuned) Top-4</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.956922</td>\n",
       "      <td>0.629204</td>\n",
       "      <td>0.992489</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "      <td>{'maxIter': 100, 'regParam': 0.0, 'threshold':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest (Tuned) Top-4</td>\n",
       "      <td>0.743979</td>\n",
       "      <td>0.842508</td>\n",
       "      <td>0.629268</td>\n",
       "      <td>0.992376</td>\n",
       "      <td>0.021164</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>60</td>\n",
       "      <td>74</td>\n",
       "      <td>{'featureSubsetStrategy': 'auto', 'maxDepth': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC (Tuned) Top-3</td>\n",
       "      <td>0.937994</td>\n",
       "      <td>0.956922</td>\n",
       "      <td>0.627644</td>\n",
       "      <td>0.992104</td>\n",
       "      <td>0.031987</td>\n",
       "      <td>0.141791</td>\n",
       "      <td>19</td>\n",
       "      <td>115</td>\n",
       "      <td>{'maxIter': 100, 'regParam': 0.0, 'threshold':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression (Tuned) Top-3</td>\n",
       "      <td>0.681165</td>\n",
       "      <td>0.799677</td>\n",
       "      <td>0.627857</td>\n",
       "      <td>0.992093</td>\n",
       "      <td>0.018884</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>67</td>\n",
       "      <td>67</td>\n",
       "      <td>{'elasticNetParam': 0.0, 'maxIter': 100, 'regP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Logistic Regression (Tuned) Top-5</td>\n",
       "      <td>0.626348</td>\n",
       "      <td>0.759518</td>\n",
       "      <td>0.621696</td>\n",
       "      <td>0.991993</td>\n",
       "      <td>0.017737</td>\n",
       "      <td>0.552239</td>\n",
       "      <td>74</td>\n",
       "      <td>60</td>\n",
       "      <td>{'elasticNetParam': 0.0, 'maxIter': 100, 'regP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression (Tuned) Top-6</td>\n",
       "      <td>0.628145</td>\n",
       "      <td>0.760894</td>\n",
       "      <td>0.616994</td>\n",
       "      <td>0.991902</td>\n",
       "      <td>0.017590</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>73</td>\n",
       "      <td>61</td>\n",
       "      <td>{'elasticNetParam': 0.0, 'maxIter': 100, 'regP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Linear SVC (Tuned) Top-6</td>\n",
       "      <td>0.677301</td>\n",
       "      <td>0.796927</td>\n",
       "      <td>0.623519</td>\n",
       "      <td>0.991887</td>\n",
       "      <td>0.018926</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>68</td>\n",
       "      <td>66</td>\n",
       "      <td>{'maxIter': 100, 'regParam': 0.0, 'threshold':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest (Tuned) Top-5</td>\n",
       "      <td>0.757189</td>\n",
       "      <td>0.851163</td>\n",
       "      <td>0.615763</td>\n",
       "      <td>0.991858</td>\n",
       "      <td>0.019101</td>\n",
       "      <td>0.380597</td>\n",
       "      <td>51</td>\n",
       "      <td>83</td>\n",
       "      <td>{'featureSubsetStrategy': 'auto', 'maxDepth': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logistic Regression (Tuned) Top-12</td>\n",
       "      <td>0.620417</td>\n",
       "      <td>0.755084</td>\n",
       "      <td>0.613511</td>\n",
       "      <td>0.991829</td>\n",
       "      <td>0.016777</td>\n",
       "      <td>0.529851</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>{'elasticNetParam': 0.0, 'maxIter': 100, 'regP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 model  accuracy        f1       auc  \\\n",
       "3    Logistic Regression (Tuned) Top-4  0.663731  0.787251  0.627279   \n",
       "5             Linear SVC (Tuned) Top-4  0.937994  0.956922  0.629204   \n",
       "4          Random Forest (Tuned) Top-4  0.743979  0.842508  0.629268   \n",
       "2             Linear SVC (Tuned) Top-3  0.937994  0.956922  0.627644   \n",
       "0    Logistic Regression (Tuned) Top-3  0.681165  0.799677  0.627857   \n",
       "6    Logistic Regression (Tuned) Top-5  0.626348  0.759518  0.621696   \n",
       "9    Logistic Regression (Tuned) Top-6  0.628145  0.760894  0.616994   \n",
       "11            Linear SVC (Tuned) Top-6  0.677301  0.796927  0.623519   \n",
       "7          Random Forest (Tuned) Top-5  0.757189  0.851163  0.615763   \n",
       "12  Logistic Regression (Tuned) Top-12  0.620417  0.755084  0.613511   \n",
       "\n",
       "      pr_auc  loyalty_precision  loyalty_recall  caught_loyal  missed_loyal  \\\n",
       "3   0.992633           0.017647        0.492537            66            68   \n",
       "5   0.992489           0.031987        0.141791            19           115   \n",
       "4   0.992376           0.021164        0.447761            60            74   \n",
       "2   0.992104           0.031987        0.141791            19           115   \n",
       "0   0.992093           0.018884        0.500000            67            67   \n",
       "6   0.991993           0.017737        0.552239            74            60   \n",
       "9   0.991902           0.017590        0.544776            73            61   \n",
       "11  0.991887           0.018926        0.507463            68            66   \n",
       "7   0.991858           0.019101        0.380597            51            83   \n",
       "12  0.991829           0.016777        0.529851            71            63   \n",
       "\n",
       "                                          best_params  \n",
       "3   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
       "5   {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
       "4   {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n",
       "2   {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
       "0   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
       "6   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
       "9   {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  \n",
       "11  {'maxIter': 100, 'regParam': 0.0, 'threshold':...  \n",
       "7   {'featureSubsetStrategy': 'auto', 'maxDepth': ...  \n",
       "12  {'elasticNetParam': 0.0, 'maxIter': 100, 'regP...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_leaderboard = pd.DataFrame(experiment_results)\n",
    "final_leaderboard = final_leaderboard.sort_values(by=\"pr_auc\", ascending=False)\n",
    "\n",
    "# 3. Save to CSV\n",
    "final_leaderboard.to_csv(\"Result_Data/3.hyperparameters_top_k_features.csv\", index=False)\n",
    "\n",
    "print(\"Success! Your experiment results have been saved to 'churn_model_experiments_final.csv'\")\n",
    "\n",
    "display(final_leaderboard.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c25a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
