{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "480ab687",
   "metadata": {},
   "source": [
    "# My Workflow\n",
    "### Overall Pipeline\n",
    "\n",
    "**Source → Read → Inspect → Clean → Integrate → Filter → Feature Engineering → Modeling-Ready Dataset**\n",
    "\n",
    "---\n",
    "\n",
    "### Data Sources (Raw Layer)\n",
    "\n",
    "* Multiple **CSV files** from the **Olist Brazilian E-Commerce dataset**\n",
    "* Files:\n",
    "\n",
    "  1. Orders\n",
    "  2. Customers\n",
    "  3. Order items\n",
    "  4. Payments\n",
    "  5. Reviews\n",
    "\n",
    " *Goal:* Work with realistic multi-table transactional data\n",
    "\n",
    "---\n",
    "\n",
    "### Data Ingestion (Read Layer)\n",
    "\n",
    "* Initialized **SparkSession**\n",
    "* Loaded all CSV files into **PySpark DataFrames**\n",
    "* Enabled schema inference & headers\n",
    "\n",
    " *Goal:* Bring raw data into Spark for scalable processing\n",
    "\n",
    "---\n",
    "\n",
    "### Data Understanding & Validation\n",
    "\n",
    "* Printed **schemas**\n",
    "* Checked **row/column counts** using a custom `checkShape()` function\n",
    "* Verified data consistency across tables\n",
    "\n",
    " *Goal:* Ensure data quality before transformations\n",
    "\n",
    "---\n",
    "\n",
    "### Data Cleaning (Trusted Raw Layer)\n",
    "\n",
    "* Selected **relevant columns only**\n",
    "* Removed unnecessary or noisy attributes\n",
    "* Feature aggregation\n",
    "* Feature Engineering (Basic)\n",
    "\n",
    " *Goal:* Reduce noise and keep business-relevant attributes\n",
    "\n",
    "---\n",
    "\n",
    "### Data Integration (Core Transformation)\n",
    "\n",
    "* Performed **multiple joins**:\n",
    "\n",
    "  * Orders ↔ Customers\n",
    "  * Orders ↔ Order Items\n",
    "  * Orders ↔ Payments\n",
    "  * Orders ↔ Reviews\n",
    "* Built a **master transactional DataFrame**\n",
    "\n",
    " *Goal:* Create a single source of truth per customer\n",
    "\n",
    "---\n",
    "\n",
    "### Time Window Filtering\n",
    "\n",
    "* Defined a **snapshot date**\n",
    "* Filtered data of **flast 180 days**\n",
    "\n",
    " *Goal:* Simulate real ML conditions\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Engineering (RFM-Based)\n",
    "\n",
    "Created **customer-level features**:\n",
    "\n",
    "* **Recency** → Days since last purchase\n",
    "* **Frequency** → Number of orders\n",
    "* **Monetary** → Total spending\n",
    "* **Total Items Volume**\n",
    "* **Customer Location** (city, state, zip)\n",
    "* **Last Purchase Date**\n",
    "\n",
    " *Goal:* Convert transactional data into behavioral signals\n",
    "\n",
    "---\n",
    "\n",
    "### Level Engineering (Aggregation)\n",
    "\n",
    "* Converted **order-level data → customer-level data**\n",
    "* Used `groupBy(customer_unique_id)`\n",
    "* Aggregated metrics using `sum`, `count`, `max`\n",
    "\n",
    " *Goal:* One row = one customer (ML-ready)\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Selection & Final Dataset\n",
    "\n",
    "* Dropped IDs like `order_id`\n",
    "* Kept only **predictive features**\n",
    "* Produced `customer_training_df`\n",
    "\n",
    " *Goal:* Clean, compact dataset for churn modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f8dd2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, countDistinct\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder \\\n",
    ".appName(\"Olist Customer Churn\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f268915f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Schema\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- order_delivered_carrier_date: string (nullable = true)\n",
      " |-- order_delivered_customer_date: string (nullable = true)\n",
      " |-- order_estimated_delivery_date: string (nullable = true)\n",
      "\n",
      "Customer Schema\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Load CSV Files\n",
    "# =========================\n",
    "orders_path = \"data/olist_orders_dataset.csv\"\n",
    "customers_path = \"data/olist_customers_dataset.csv\"\n",
    "order_items_path = \"data/olist_order_items_dataset.csv\"\n",
    "order_payment_path = \"data/olist_order_payments_dataset.csv\"\n",
    "order_reviews_path = \"data/olist_order_reviews_dataset.csv\"\n",
    "\n",
    "orders_df = spark.read.csv(\n",
    "    orders_path,\n",
    "    header= True,\n",
    "    inferSchema= True,\n",
    "    multiLine=True,\n",
    "    escape='\"',\n",
    "    quote='\"'\n",
    ")\n",
    "\n",
    "customers_df = spark.read.csv(\n",
    "    customers_path,\n",
    "    header = True,\n",
    "    inferSchema = True,\n",
    "    multiLine=True,\n",
    "    escape='\"',\n",
    "    quote='\"'\n",
    ")\n",
    "\n",
    "order_items_df = spark.read.csv(\n",
    "    order_items_path,\n",
    "    header= True,\n",
    "    inferSchema= True,\n",
    "    multiLine=True,\n",
    "    escape='\"',\n",
    "    quote='\"'\n",
    ")\n",
    "\n",
    "order_payment_df = spark.read.csv(\n",
    "    order_payment_path,\n",
    "    header= True,\n",
    "    inferSchema= True,\n",
    "    multiLine=True,\n",
    "    escape='\"',\n",
    "    quote='\"'\n",
    ")\n",
    "\n",
    "order_reviews_df = spark.read.csv(\n",
    "    order_reviews_path,\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    "    multiLine=True,\n",
    "    escape='\"',\n",
    "    quote='\"'\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Check Schema\n",
    "# =========================\n",
    "\n",
    "print(\"Order Schema\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "print(\"Customer Schema\")\n",
    "customers_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "320f3319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order Item Schema\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n",
      "Order Payment Schema\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_sequential: integer (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: integer (nullable = true)\n",
      " |-- payment_value: double (nullable = true)\n",
      "\n",
      "Order Reviews Schema\n",
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- review_score: integer (nullable = true)\n",
      " |-- review_comment_title: string (nullable = true)\n",
      " |-- review_comment_message: string (nullable = true)\n",
      " |-- review_creation_date: string (nullable = true)\n",
      " |-- review_answer_timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Order Item Schema\")\n",
    "order_items_df.printSchema()\n",
    "\n",
    "print(\"Order Payment Schema\")\n",
    "order_payment_df.printSchema()\n",
    "\n",
    "print(\"Order Reviews Schema\")\n",
    "order_reviews_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcf4ac",
   "metadata": {},
   "source": [
    "**Usefull Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "42f3008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================\n",
    "# Check Shape --> Rows and Columns\n",
    "# ==================================\n",
    "\n",
    "def  checkShape(df, df_name):\n",
    "    rows = df.count()\n",
    "    cols = len(df.columns)\n",
    "    print(f\"The shape of {df_name} is: {rows} rows by {cols} columns\")\n",
    "\n",
    "# ==================================\n",
    "# Check Unique --> Unique or Not\n",
    "# ==================================\n",
    "\n",
    "def check_uniqueness(df, column_name, df_name):\n",
    "    total_count = df.count()\n",
    "    unique_count = df.select(column_name).distinct().count()\n",
    "    print(f\"--- Checking Uniqueness for {df_name} ---\")\n",
    "    print(f\"Total Rows: {total_count}\")\n",
    "    print(f\"Unique {column_name}s: {unique_count}\")\n",
    "\n",
    "    if total_count == unique_count:\n",
    "        print(f\"SUCCESS: {column_name} is a Unique Key in {df_name}.\")\n",
    "    else:\n",
    "        print(f\"WARNING: {column_name} is NOT unique in {df_name}. Duplicates found!\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# ==================================\n",
    "# Check Null\n",
    "# ==================================\n",
    "def CheckNull(df):\n",
    "    df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n",
    "\n",
    "# ==================================\n",
    "# Check Duplicate\n",
    "# ==================================\n",
    "\n",
    "def CheckDuplicate(df, df_name):\n",
    "    total_rows = df.count()\n",
    "    unique_orders = df.select(\"order_id\").distinct().count()\n",
    "    print(f\"--- Duplicate Audit for {df_name} ---\")\n",
    "    print(f\"Total Rows: {total_rows}\")\n",
    "    print(f\"Unique Order IDs: {unique_orders}\")\n",
    "\n",
    "    if total_rows == unique_orders:\n",
    "        print(\"SUCCESS: Each order_id is now unique. No duplicates!\")\n",
    "    else:\n",
    "        print(f\"WARNING: You have {total_rows - unique_orders} duplicate rows. Check your groupBy logic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "fddbcf82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of orders_df is: 99441 rows by 8 columns\n",
      "The shape of customers_df is: 99441 rows by 5 columns\n",
      "The shape of order_items_df is: 112650 rows by 7 columns\n",
      "The shape of order_payment_df is: 103886 rows by 5 columns\n",
      "The shape of order_reviews_df is: 99224 rows by 7 columns\n"
     ]
    }
   ],
   "source": [
    "checkShape(orders_df, \"orders_df\")\n",
    "checkShape(customers_df, \"customers_df\")\n",
    "checkShape(order_items_df, \"order_items_df\")\n",
    "checkShape(order_payment_df, \"order_payment_df\")\n",
    "checkShape(order_reviews_df, \"order_reviews_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f23684",
   "metadata": {},
   "source": [
    "*Check Uniqueness*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "b2ec1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Uniqueness for Customers ---\n",
      "Total Rows: 99441\n",
      "Unique customer_ids: 99441\n",
      "SUCCESS: customer_id is a Unique Key in Customers.\n",
      "----------------------------------------\n",
      "--- Checking Uniqueness for Orders ---\n",
      "Total Rows: 99441\n",
      "Unique order_ids: 99441\n",
      "SUCCESS: order_id is a Unique Key in Orders.\n",
      "----------------------------------------\n",
      "--- Checking Uniqueness for Order Items ---\n",
      "Total Rows: 112650\n",
      "Unique order_ids: 98666\n",
      "WARNING: order_id is NOT unique in Order Items. Duplicates found!\n",
      "----------------------------------------\n",
      "--- Checking Uniqueness for Order Payments ---\n",
      "Total Rows: 103886\n",
      "Unique order_ids: 99440\n",
      "WARNING: order_id is NOT unique in Order Payments. Duplicates found!\n",
      "----------------------------------------\n",
      "--- Checking Uniqueness for Order Reviews ---\n",
      "Total Rows: 99224\n",
      "Unique order_ids: 98673\n",
      "WARNING: order_id is NOT unique in Order Reviews. Duplicates found!\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "check_uniqueness(customers_df, \"customer_id\", \"Customers\")\n",
    "check_uniqueness(orders_df, \"order_id\", \"Orders\")\n",
    "check_uniqueness(order_items_df, \"order_id\", \"Order Items\")\n",
    "check_uniqueness(order_payment_df, \"order_id\", \"Order Payments\")\n",
    "check_uniqueness(order_reviews_df, \"order_id\", \"Order Reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6218ad20",
   "metadata": {},
   "source": [
    "### Data Cleaning and Reducing Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "80fa6159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null Value Audit for items_agg ---\n",
      "+--------+-----------+-------------+-----------+-------------------+\n",
      "|order_id|total_price|total_freight|total_items|shipping_limit_date|\n",
      "+--------+-----------+-------------+-----------+-------------------+\n",
      "|       0|          0|            0|          0|                  0|\n",
      "+--------+-----------+-------------+-----------+-------------------+\n",
      "\n",
      "--- Duplicate Audit for items_agg ---\n",
      "Total Rows: 98666\n",
      "Unique Order IDs: 98666\n",
      "SUCCESS: Each order_id is now unique. No duplicates!\n",
      "\n",
      "The Final Order Item Dataset\n",
      "+--------------------+-----------+-------------+-----------+-------------------+\n",
      "|            order_id|total_price|total_freight|total_items|shipping_limit_date|\n",
      "+--------------------+-----------+-------------+-----------+-------------------+\n",
      "|00018f77f2f0320c5...|      239.9|        19.93|          1|     5/3/2017 11:05|\n",
      "|00042b26cf59d7ce6...|      199.9|        18.14|          1|    2/13/2017 13:57|\n",
      "|00054e8431b9d7675...|       19.9|        11.85|          1|   12/14/2017 12:10|\n",
      "|0006ec9db01a64e59...|       74.0|        23.32|          1|    7/26/2018 17:24|\n",
      "|000aed2e25dbad2f9...|      144.0|         8.77|          1|    5/16/2018 20:57|\n",
      "+--------------------+-----------+-------------+-----------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Order Items Table\n",
    "# ==================================\n",
    "\n",
    "#Step 1: Feature Aggregation\n",
    "items_agg = order_items_df.groupBy('order_id').agg(\n",
    "    F.sum(\"price\").alias(\"total_price\"),\n",
    "    F.sum(\"freight_value\").alias(\"total_freight\"),\n",
    "    F.count(\"product_id\").alias(\"total_items\"),\n",
    "    F.first(\"shipping_limit_date\").alias(\"shipping_limit_date\")\n",
    ")\n",
    "\n",
    "# Step 2: Check for Null Values in each column\n",
    "print(\"--- Null Value Audit for items_agg ---\")\n",
    "CheckNull(items_agg)\n",
    "\n",
    "# Step 3: Check for Duplicate order_ids\n",
    "CheckDuplicate(items_agg, \"items_agg\")\n",
    "\n",
    "\n",
    "print(\"\\nThe Final Order Item Dataset\")\n",
    "items_agg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c7627f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null Value Audit for payment_agg ---\n",
      "+--------+-------------------+----------------+--------------------+--------------------+\n",
      "|order_id|total_order_payment|max_installments|payment_method_count|primary_payment_type|\n",
      "+--------+-------------------+----------------+--------------------+--------------------+\n",
      "|       0|                  0|               0|                   0|                   0|\n",
      "+--------+-------------------+----------------+--------------------+--------------------+\n",
      "\n",
      "--- Duplicate Audit for payment_agg ---\n",
      "Total Rows: 99440\n",
      "Unique Order IDs: 99440\n",
      "SUCCESS: Each order_id is now unique. No duplicates!\n",
      "\n",
      "The Final Order Payment Dataset\n",
      "+--------------------+-------------------+----------------+--------------------+--------------------+\n",
      "|            order_id|total_order_payment|max_installments|payment_method_count|primary_payment_type|\n",
      "+--------------------+-------------------+----------------+--------------------+--------------------+\n",
      "|00018f77f2f0320c5...|             259.83|               3|                   1|         credit_card|\n",
      "|00042b26cf59d7ce6...|             218.04|               3|                   1|         credit_card|\n",
      "|00054e8431b9d7675...|              31.75|               1|                   1|         credit_card|\n",
      "|0006ec9db01a64e59...|              97.32|               4|                   1|         credit_card|\n",
      "|000aed2e25dbad2f9...|             152.77|               1|                   1|         credit_card|\n",
      "+--------------------+-------------------+----------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Order Payment Table\n",
    "# ==================================\n",
    "\n",
    "# Step 1: Feature Aggregation\n",
    "payment_agg = order_payment_df.groupBy('order_id').agg(\n",
    "    F.sum(\"payment_value\").alias(\"total_order_payment\"),\n",
    "    F.max(\"payment_installments\").alias(\"max_installments\"),\n",
    "    F.count(\"payment_sequential\").alias(\"payment_method_count\"),\n",
    "    F.first(\"payment_type\").alias(\"primary_payment_type\")\n",
    ")\n",
    "\n",
    "# Step 2: Check for Null Values in each column\n",
    "print(\"--- Null Value Audit for payment_agg ---\")\n",
    "CheckNull(payment_agg)\n",
    "\n",
    "# Step 3: Check for Duplicate order_ids\n",
    "CheckDuplicate(payment_agg, \"payment_agg\")\n",
    "\n",
    "print(\"\\nThe Final Order Payment Dataset\")\n",
    "payment_agg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ed1ca850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null Value Audit for reviews_agg ---\n",
      "+--------+----------------+----------------+-------------------+\n",
      "|order_id|Avg_review_score|Min_review_score|Total_Review_Number|\n",
      "+--------+----------------+----------------+-------------------+\n",
      "|       0|               0|               0|                  0|\n",
      "+--------+----------------+----------------+-------------------+\n",
      "\n",
      "--- Duplicate Audit for reviews_agg ---\n",
      "Total Rows: 98673\n",
      "Unique Order IDs: 98673\n",
      "SUCCESS: Each order_id is now unique. No duplicates!\n",
      "\n",
      "The Final Order Payment Dataset\n",
      "+--------------------+----------------+----------------+-------------------+\n",
      "|            order_id|Avg_review_score|Min_review_score|Total_Review_Number|\n",
      "+--------------------+----------------+----------------+-------------------+\n",
      "|e239d280236cdd3c4...|             5.0|               5|                  1|\n",
      "|d451da9b109e1786f...|             5.0|               5|                  1|\n",
      "|0de5dbc9f32267616...|             4.0|               4|                  1|\n",
      "|251f0a3981c4a8cb8...|             5.0|               5|                  1|\n",
      "|f63a31c3349b87273...|             5.0|               5|                  2|\n",
      "+--------------------+----------------+----------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# ==================================\n",
    "# Order Reviews Table\n",
    "# ==================================\n",
    "\n",
    "# Steps 1: Aggregating Reviews\n",
    "reviews_agg = order_reviews_df.groupBy(\"order_id\").agg(\n",
    "    F.avg(\"review_score\").alias(\"Avg_review_score\"),\n",
    "    F.min(\"review_score\").alias(\"Min_review_score\"),\n",
    "    F.count(\"review_id\").alias(\"Total_Review_Number\")\n",
    ")\n",
    "\n",
    "# Step 2: Check for Null Values in each column\n",
    "print(\"--- Null Value Audit for reviews_agg ---\")\n",
    "CheckNull(reviews_agg)\n",
    "\n",
    "# Step 3: Check for Duplicate order_ids\n",
    "CheckDuplicate(reviews_agg, \"reviews_agg\")\n",
    "\n",
    "print(\"\\nThe Final Order Payment Dataset\")\n",
    "reviews_agg.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f55c50a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of orders_df is: 99441 rows by 8 columns\n",
      "The shape of customers_df is: 99441 rows by 5 columns\n",
      "The shape of order_items_df is: 98666 rows by 5 columns\n",
      "The shape of payment_agg is: 99440 rows by 5 columns\n",
      "The shape of reviews_agg is: 98673 rows by 4 columns\n"
     ]
    }
   ],
   "source": [
    "checkShape(orders_df, \"orders_df\")\n",
    "checkShape(customers_df, \"customers_df\")\n",
    "checkShape(items_agg, \"order_items_df\")\n",
    "checkShape(payment_agg, \"payment_agg\")\n",
    "checkShape(reviews_agg, \"reviews_agg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5f3a6b",
   "metadata": {},
   "source": [
    "**Three Tables Schema Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "34bd8eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Dataset of Order Item\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- total_freight: double (nullable = true)\n",
      " |-- total_items: long (nullable = false)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      "\n",
      "Final Dataset of Order Payment\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- total_order_payment: double (nullable = true)\n",
      " |-- max_installments: integer (nullable = true)\n",
      " |-- payment_method_count: long (nullable = false)\n",
      " |-- primary_payment_type: string (nullable = true)\n",
      "\n",
      "Final Dataset of Order Review\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- Avg_review_score: double (nullable = true)\n",
      " |-- Min_review_score: integer (nullable = true)\n",
      " |-- Total_Review_Number: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Dataset of Order Item\")\n",
    "items_agg.printSchema()\n",
    "\n",
    "print(\"Final Dataset of Order Payment\")\n",
    "payment_agg.printSchema()\n",
    "\n",
    "print(\"Final Dataset of Order Review\")\n",
    "reviews_agg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725de226",
   "metadata": {},
   "source": [
    "### Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0150bb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of master_df is: 99441 rows by 23 columns\n",
      "Final Dataset Schema\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- order_delivered_carrier_date: string (nullable = true)\n",
      " |-- order_delivered_customer_date: string (nullable = true)\n",
      " |-- order_estimated_delivery_date: string (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- total_freight: double (nullable = true)\n",
      " |-- total_items: long (nullable = true)\n",
      " |-- shipping_limit_date: string (nullable = true)\n",
      " |-- total_order_payment: double (nullable = true)\n",
      " |-- max_installments: integer (nullable = true)\n",
      " |-- payment_method_count: long (nullable = true)\n",
      " |-- primary_payment_type: string (nullable = true)\n",
      " |-- Avg_review_score: double (nullable = true)\n",
      " |-- Min_review_score: integer (nullable = true)\n",
      " |-- Total_Review_Number: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Joining Tables --> Inner → left → left → lest → master dataframe\n",
    "# ============================================================================\n",
    "# Step 1: Join Five Tables\n",
    "master_df = customers_df.join(orders_df, \"customer_id\", \"inner\")\n",
    "master_df = master_df.join(items_agg, \"order_id\", \"left\")\n",
    "master_df = master_df.join(payment_agg, \"order_id\", \"left\")\n",
    "master_df = master_df.join(reviews_agg, \"order_id\", \"left\")\n",
    "\n",
    "# Step 2: Check Rows and Columns\n",
    "checkShape(master_df, \"master_df\")\n",
    "\n",
    "# Step 3: DataFrame Schema\n",
    "print(\"Final Dataset Schema\")\n",
    "master_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "60766b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Master Dataset Schema\n",
      "+--------------------+--------------------+--------------------+------------------------+--------------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+\n",
      "|            order_id|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|total_price|total_freight|total_items|shipping_limit_date|total_order_payment|max_installments|payment_method_count|primary_payment_type|Avg_review_score|Min_review_score|Total_Review_Number|\n",
      "+--------------------+--------------------+--------------------+------------------------+--------------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                    3149|           sao paulo|            SP|   delivered|         10/2/2017 10:56|  10/2/2017 11:07|             10/4/2017 19:55|             10/10/2017 21:25|              10/18/2017 0:00|      29.99|         8.72|          1|    10/6/2017 11:07|              38.71|               1|                   3|         credit_card|             4.0|               4|                  1|\n",
      "|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|af07308b275d755c9...|                   47813|           barreiras|            BA|   delivered|         7/24/2018 20:41|   7/26/2018 3:24|             7/26/2018 14:31|               8/7/2018 15:27|               8/13/2018 0:00|      118.7|        22.76|          1|     7/30/2018 3:24|             141.46|               1|                   1|              boleto|             4.0|               4|                  1|\n",
      "|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|3a653a41f6f9fc3d2...|                   75265|          vianopolis|            GO|   delivered|           8/8/2018 8:38|    8/8/2018 8:55|              8/8/2018 13:50|              8/17/2018 18:06|                9/4/2018 0:00|      159.9|        19.22|          1|     8/13/2018 8:55|             179.12|               3|                   1|         credit_card|             5.0|               5|                  1|\n",
      "|949d5b44dbf5de918...|f88197465ea7920ad...|7c142cf63193a1473...|                   59296|sao goncalo do am...|            RN|   delivered|        11/18/2017 19:28| 11/18/2017 19:45|            11/22/2017 13:39|               12/2/2017 0:28|              12/15/2017 0:00|       45.0|         27.2|          1|   11/23/2017 19:45|               72.2|               1|                   1|         credit_card|             5.0|               5|                  1|\n",
      "|ad21c59c0840e6cb8...|8ab97904e6daea886...|72632f0f9dd73dfee...|                    9195|         santo andre|            SP|   delivered|         2/13/2018 21:18|  2/13/2018 22:20|             2/14/2018 19:46|              2/16/2018 18:17|               2/26/2018 0:00|       19.9|         8.72|          1|    2/19/2018 20:31|              28.62|               1|                   1|         credit_card|             5.0|               5|                  1|\n",
      "+--------------------+--------------------+--------------------+------------------------+--------------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Final DataFrame View\n",
    "print(\"Final Master Dataset Schema\")\n",
    "master_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ed30e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value Audit for master_df\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+\n",
      "|order_id|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|total_price|total_freight|total_items|shipping_limit_date|total_order_payment|max_installments|payment_method_count|primary_payment_type|Avg_review_score|Min_review_score|Total_Review_Number|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+\n",
      "|       0|          0|                 0|                       0|            0|             0|           0|                       0|              160|                        1783|                         2965|                            0|        775|          775|        775|                775|                  1|               1|                   1|                   1|             768|             768|                768|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+\n",
      "\n",
      "Check Duplicate\n",
      "--- Duplicate Audit for master_df ---\n",
      "Total Rows: 99441\n",
      "Unique Order IDs: 99441\n",
      "SUCCESS: Each order_id is now unique. No duplicates!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Check Null Values\n",
    "print(\"Null Value Audit for master_df\")\n",
    "CheckNull(master_df)\n",
    "\n",
    "# Step 5: Check Duplicate\n",
    "print(\"Check Duplicate\")\n",
    "CheckDuplicate(master_df, \"master_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e7bf7",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "576a1143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Casting to Tilestamp the Final Dataset\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- total_freight: double (nullable = true)\n",
      " |-- total_items: long (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- total_order_payment: double (nullable = true)\n",
      " |-- max_installments: integer (nullable = true)\n",
      " |-- payment_method_count: long (nullable = true)\n",
      " |-- primary_payment_type: string (nullable = true)\n",
      " |-- Avg_review_score: double (nullable = true)\n",
      " |-- Min_review_score: integer (nullable = true)\n",
      " |-- Total_Review_Number: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Timestamp Casting\n",
    "# ==========================\n",
    "date_format = \"M/d/yyyy H:mm\"\n",
    "date_cols = [\n",
    "    \"order_purchase_timestamp\", \"order_approved_at\", \n",
    "    \"order_delivered_carrier_date\", \"order_delivered_customer_date\", \n",
    "    \"order_estimated_delivery_date\", \"shipping_limit_date\"\n",
    "]\n",
    "\n",
    "master_df_date = master_df\n",
    "for col_name in date_cols:\n",
    "    master_df_date = master_df_date.withColumn(col_name, F.to_timestamp(col_name, date_format))\n",
    "\n",
    "print(\"After Casting to Tilestamp the Final Dataset\")\n",
    "master_df_date.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fd4bf2",
   "metadata": {},
   "source": [
    "### Handling Missing Values\n",
    "\n",
    "**Why did I drop numm value of *total_items* and *total_order_payment* features?**\n",
    "**Answer:**\n",
    "A null value in *total_items* or *total_order_payment* indicates that the customer did not purchase anything. In my opinion, imputing values for these nulls would mislead the model by providing false information.\n",
    "\n",
    "---\n",
    "\n",
    "**Why did I choose to impute values for *Avg_review_score*, *Min_review_score*, and *Total_Review_Number*?**\n",
    "**Answer:**\n",
    "If a customer does not provide any review, it means they have no expressed opinion about their purchase. Assigning a value of 3 represents a neutral sentiment—neither positive nor negative.\n",
    "\n",
    "---\n",
    "\n",
    "**What about *order_delivered_customer_date*, *order_approved_at*, *order_estimated_delivery_date* and  *order_delivered_carrier_date*?**\n",
    "**Answer:**\n",
    "If the dates are missing, it make sence that the order was either not delivered or not approved. Therefore, we convert these missing values into meaningful indicator features and delete them;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "36f64973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Null Value Audit for master_df ---\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+------------+-----------+-------------------+\n",
      "|order_id|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|order_status|order_purchase_timestamp|total_price|total_freight|total_items|shipping_limit_date|total_order_payment|max_installments|payment_method_count|primary_payment_type|Avg_review_score|Min_review_score|Total_Review_Number|is_delivered|is_approved|delivery_delay_days|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+------------+-----------+-------------------+\n",
      "|       0|          0|                 0|                       0|            0|             0|           0|                       0|          0|            0|          0|                  0|                  0|               0|                   0|                   0|               0|               0|                  0|           0|          0|                  0|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------+-------------+-----------+-------------------+-------------------+----------------+--------------------+--------------------+----------------+----------------+-------------------+------------+-----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Timestamp Casting\n",
    "# ==========================\n",
    "# Step 1: DROP the logical impossibilities (Ghost Records)\n",
    "master_df_drop = master_df_date.dropna(subset=[\"total_items\", \"total_order_payment\"])\n",
    "\n",
    "master_df_fillna = master_df_drop.fillna({\n",
    "    \"Avg_review_score\": 3.0,\n",
    "    \"Min_review_score\": 3.0,\n",
    "    \"Total_Review_Number\": 0\n",
    "})\n",
    "\n",
    "# Step 2: Data Imputation\n",
    "master_df_update = master_df_fillna.withColumn(\n",
    "    \"is_delivered\",\n",
    "    F.when(F.col(\"order_delivered_customer_date\").isNotNull(), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_approved\",\n",
    "    F.when(F.col(\"order_approved_at\").isNotNull(), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"delivery_delay_days\",\n",
    "    F.datediff(\"order_delivered_customer_date\", \"order_estimated_delivery_date\")     # Calculate the raw difference\n",
    ").withColumn(\n",
    "    \"delivery_delay_days\",                                                           # Apply the clipping logic\n",
    "    F.when(F.col(\"delivery_delay_days\") > 30, 30)                                    # Cap late/nulls at 30\n",
    "     .when(F.col(\"delivery_delay_days\") < -30, -30)                                  # Cap early at -30\n",
    "     .otherwise(F.col(\"delivery_delay_days\"))\n",
    ").fillna(\n",
    "    {\"delivery_delay_days\": 31}                                                      # Assign a specific penalty for nulls (Non-delivered)\n",
    ").drop(\"order_approved_at\", \"order_delivered_carrier_date\", \"order_estimated_delivery_date\", \"order_delivered_customer_date\")\n",
    "\n",
    "# Step 3: Check Null value\n",
    "print(\"--- Null Value Audit for master_df ---\")\n",
    "CheckNull(master_df_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455dc1dd",
   "metadata": {},
   "source": [
    "### Choose Time Frame\n",
    "\n",
    "---\n",
    "\n",
    "This time frame, I choose, is strategically chosen to balance seasonal patterns with realistic customer behavior in an e-commerce context. By using a **12-month feature window**, the model captures a full annual cycle, ensuring that seasonal peaks—such as Black Friday, Christmas, and local Brazilian holidays—are reflected in the customer’s behavioral features like frequency and spending. This year-long history provides a robust baseline for establishing \"normal\" behavior. The subsequent **6-month label window** is critical for a \"low-frequency\" retail environment; because customers do not purchase every week, a shorter window might incorrectly label a slow-but-loyal buyer as \"churned.\" A 6-month period provides enough time to realistically observe whether a customer intends to return, while the overall **18-month snapshot** fits perfectly within your two-year dataset, leaving the model with enough relevant, recent data to make accurate predictions without being diluted by outdated trends from years prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1004e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset End: 2018-09-03 09:06:00\n",
      "Split Point (Prediction Start): 2018-03-07 09:06:00\n",
      "Feature History Start: 2017-03-07 09:06:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# ==========================================\n",
    "# Define the 12-month and 6-month Windows\n",
    "# ==========================================\n",
    "\n",
    "# STEP 1: Get the absolute last date in your dataset\n",
    "max_date = master_df_update.select(F.max(\"order_purchase_timestamp\")).collect()[0][0]\n",
    "\n",
    "# STEP 2: Split date is 6 months (180 days) before the end\n",
    "split_date = max_date - timedelta(days=180)\n",
    "\n",
    "# STEP 3: Start date for features is 12 months (365 days) before the split\n",
    "start_date = split_date - timedelta(days=365)\n",
    "\n",
    "print(f\"Dataset End: {max_date}\")\n",
    "print(f\"Split Point (Prediction Start): {split_date}\")\n",
    "print(f\"Feature History Start: {start_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba58b5",
   "metadata": {},
   "source": [
    "### Feature Engineering (RFM-Based)\n",
    "Extracting *Recency*, *Frequency*, *Monetary* Features consider as Simple and interpretable for customer churn, which called as RFM-based churn modeling.\n",
    "\n",
    "**Why each of the excluded features is not kept?**\n",
    "* `order_id`: Transaction ID → replaced by `frequency`\n",
    "* `customer_id`: Duplicate of `customer_unique_id`\n",
    "* `order_purchase_timestamp`: Used to compute recency → keep aggregated only\n",
    "* `order_approved_at`: Used to compute is_approved → aggregated feature sufficient\n",
    "* `shipping_limit_date`, `order_estimated_delivery_date` and `order_delivered_customer_date`: Used for `delivery_delay_days`\n",
    "* `total_price`, `total_freight` and `total_order_payment`: Captured by `Monetary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "aa839567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- recency: integer (nullable = true)\n",
      " |-- frequency: long (nullable = false)\n",
      " |-- monetary: double (nullable = true)\n",
      " |-- zip_code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- total_items_volume: long (nullable = true)\n",
      " |-- max_delivery_delay: integer (nullable = true)\n",
      " |-- is_delivered: integer (nullable = true)\n",
      " |-- is_approved: integer (nullable = true)\n",
      " |-- payment_method_count: long (nullable = true)\n",
      " |-- primary_payment_type: string (nullable = true)\n",
      " |-- max_installments: integer (nullable = true)\n",
      " |-- avg_satisfaction: double (nullable = true)\n",
      " |-- min_review_score: integer (nullable = true)\n",
      " |-- total_reviews_given: long (nullable = true)\n",
      " |-- label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Temporal Feature Engineering\n",
    "# ==========================================\n",
    "\n",
    "# STEP 1: Filter for Features (The 12-month \"Past\")\n",
    "obs_df = master_df_update.filter(\n",
    "    (F.col(\"order_purchase_timestamp\") >= start_date) & \n",
    "    (F.col(\"order_purchase_timestamp\") < split_date)\n",
    ")\n",
    "\n",
    "# STEP 2: Customer Roll-Up (Feature Engineering)\n",
    "customer_features = obs_df.groupBy(\"customer_unique_id\").agg(\n",
    "    F.datediff(F.lit(split_date), F.max(\"order_purchase_timestamp\")).alias(\"recency\"),\n",
    "    F.count(\"order_id\").alias(\"frequency\"),\n",
    "    F.sum(\"total_order_payment\").alias(\"monetary\"),\n",
    "    F.first(\"customer_zip_code_prefix\", ignorenulls=True).alias(\"zip_code\"),\n",
    "    F.first(\"customer_city\", ignorenulls=True).alias(\"city\"),\n",
    "    F.first(\"customer_state\", ignorenulls=True).alias(\"state\"),\n",
    "    F.sum(\"total_items\").alias(\"total_items_volume\"),\n",
    "    F.max(\"delivery_delay_days\").alias(\"max_delivery_delay\"),\n",
    "    F.max(\"is_delivered\").alias(\"is_delivered\"),\n",
    "    F.max(\"is_approved\").alias(\"is_approved\"),\n",
    "    F.max(\"payment_method_count\").alias(\"payment_method_count\"),\n",
    "    F.first(\"primary_payment_type\", ignorenulls=True).alias(\"primary_payment_type\"),\n",
    "    F.max(\"max_installments\").alias(\"max_installments\"),\n",
    "    F.avg(\"Avg_review_score\").alias(\"avg_satisfaction\"),\n",
    "    F.min(\"Min_review_score\").alias(\"min_review_score\"),\n",
    "    F.sum(\"Total_Review_Number\").alias(\"total_reviews_given\")\n",
    ")\n",
    "\n",
    "# STEP 3: Create Labels (The 6-month \"Future\")\n",
    "future_df = master_df_update.filter(\n",
    "    (F.col(\"order_purchase_timestamp\") >= split_date) & \n",
    "    (F.col(\"order_purchase_timestamp\") <= max_date)\n",
    ")\n",
    "returned_customers = future_df.select(\"customer_unique_id\").distinct().withColumn(\"label\", F.lit(0))\n",
    "\n",
    "# STEP 4: Join and Handle Churners\n",
    "customer_training_df = customer_features.join(returned_customers, on=\"customer_unique_id\", how=\"left\")\n",
    "customer_training_df = customer_training_df.fillna({\"label\": 1})\n",
    "\n",
    "# Step 5: Final Dataset Schema\n",
    "customer_training_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "09803bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Final Data Frame is: 55245 rows by 18 columns\n",
      "+--------------------+-------+---------+--------+--------+--------------------+-----+------------------+------------------+------------+-----------+--------------------+--------------------+----------------+----------------+----------------+-------------------+-----+\n",
      "|  customer_unique_id|recency|frequency|monetary|zip_code|                city|state|total_items_volume|max_delivery_delay|is_delivered|is_approved|payment_method_count|primary_payment_type|max_installments|avg_satisfaction|min_review_score|total_reviews_given|label|\n",
      "+--------------------+-------+---------+--------+--------+--------------------+-----+------------------+------------------+------------+-----------+--------------------+--------------------+----------------+----------------+----------------+-------------------+-----+\n",
      "|0006fdc98a402fceb...|    232|        1|    29.0|   29400|       mimoso do sul|   ES|                 1|               -12|           1|          1|                   1|         credit_card|               2|             3.0|               3|                  1|    1|\n",
      "|000c8bdb58a29e711...|     85|        1|    29.0|   31555|      belo horizonte|   MG|                 1|                -9|           1|          1|                   1|         credit_card|               2|             5.0|               5|                  1|    1|\n",
      "|001f3c4211216384d...|    353|        1|   35.84|    8240|           sao paulo|   SP|                 1|               -11|           1|          1|                   1|         credit_card|               3|             3.0|               3|                  1|    1|\n",
      "|002cdf87d4c03f08f...|    127|        1|  228.67|   13183|         hortolandia|   SP|                 1|                14|           1|          1|                   1|         credit_card|              10|             1.0|               1|                  1|    1|\n",
      "|002d71b244beb91ca...|    289|        1|  130.56|   18900|santa cruz do rio...|   SP|                 1|               -12|           1|          1|                   1|              boleto|               1|             5.0|               5|                  1|    1|\n",
      "+--------------------+-------+---------+--------+--------+--------------------+-----+------------------+------------------+------------+-----------+--------------------+--------------------+----------------+----------------+----------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Final Dataset Audit\n",
    "checkShape(customer_training_df, \"Final Data Frame\")\n",
    "customer_training_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d03cb0",
   "metadata": {},
   "source": [
    "**Final Features Explenation**\n",
    "\n",
    "* **customer_unique_id** → Unique identifier representing an individual customer across all orders.\n",
    "* **recency** → Number of days since the customer’s most recent purchase.\n",
    "* **frequency** → Total number of orders placed by the customer.\n",
    "* **monetary** → Total amount of money spent by the customer.\n",
    "* **zip_code** → Postal code of the customer’s delivery address.\n",
    "* **city** → City where the customer is located.\n",
    "* **state** → State where the customer is located.\n",
    "* **total_items_volume** → Total number of items purchased by the customer across all orders.\n",
    "* **max_delivery_delay** → Maximum delivery delay experienced by the customer in days.\n",
    "* **is_delivered** → Indicator showing whether the customer has at least one successfully delivered order.\n",
    "* **is_approved** → Indicator showing whether the customer’s payment was approved.\n",
    "* **payment_method_count** → Number of distinct payment methods used by the customer.\n",
    "* **primary_payment_type** → Most frequently used payment method by the customer.\n",
    "* **max_installments** → Maximum number of payment installments chosen by the customer.\n",
    "* **avg_satisfaction** → Average review score given by the customer across all orders.\n",
    "* **min_review_score** → Lowest review score ever given by the customer.\n",
    "* **total_reviews_given** → Total number of reviews submitted by the customer.\n",
    "* **label** → Target variable indicating whether the customer has churned (1) or not (0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8c326179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully saved as 'h:\\OneDrive - etu.u-cergy.fr\\M2 Classes\\5. Big Data & AI__Project\\Project 3 - Brazilian E-Commerce Public Dataset by Olist\\data/finalData\\customer_training.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# =========================\n",
    "# Export the Final Dataset\n",
    "# =========================\n",
    "\n",
    "# Step 1: Define subfolder path\n",
    "data_folder = os.path.join(os.getcwd(), \"data/finalData\")\n",
    "\n",
    "# Step 2: Create folder if it doesn't exist\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Step 3: Define full CSV path\n",
    "output_csv = os.path.join(data_folder, \"customer_training.csv\")\n",
    "\n",
    "# Step 4: Save DataFrame to CSV\n",
    "customer_training_df.toPandas().to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"DataFrame successfully saved as '{output_csv}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff74728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
